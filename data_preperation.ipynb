{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f9751c",
   "metadata": {},
   "source": [
    "# Deep Learning group assignment\n",
    "Group name: Angry Birds\n",
    "\n",
    "Group members:\n",
    "- Nienke Reijnen: 2117034\n",
    "- Andrea Ciavatti: 2115635\n",
    "- Niels Boonstra: 1451294\n",
    "- Yannick Lankhorst: 2052754\n",
    "- Thom Zoomer:2059225\n",
    "- Anne Barnasconi: 2053988"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc615d47",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1df5d",
   "metadata": {},
   "source": [
    "Before running, make sure to also have installed the following packages (according to lab 8 instructions):\n",
    "- pip install imageio\n",
    "- pip install future\n",
    "- pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b751e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.transforms import functional\n",
    "from torch.utils.data import ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import itertools\n",
    "# from ultralytics import YOLO\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, device_name = (torch.device(\"cuda\"), torch.cuda.get_device_name(0)) if torch.cuda.is_available() else (torch.device(\"cpu\"), \"CPU\")\n",
    "print(f\"Device: {device}, {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f9ca2",
   "metadata": {},
   "source": [
    "## Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0c073",
   "metadata": {},
   "source": [
    "- DONE Correct implementation of data loaders for images and annotations for your specific object detection model\n",
    "- DONE Use of data augmentation techniques\n",
    "- DONE Appropriate shuffling  and batching of data\n",
    "- TO DO: Conduct an online search for relevant open-source datasets, and if you can find them, use them in your application as additional training data (to improve generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999552a",
   "metadata": {},
   "source": [
    "### The code below needs to be run on your own laptop to convert the names of the scare_crow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory and dataset splits\n",
    "base_dir = \"scarecrow_dataset\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Allowed image extensions (all lowercased for matching)\n",
    "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\nProcessing {split} split...\")\n",
    "\n",
    "    image_dir = os.path.join(base_dir, split, \"images\")\n",
    "    annotation_path = os.path.join(base_dir, split, \"annotations.json\")\n",
    "\n",
    "    # Load annotations\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    print(f\"Found {len(annotations)} annotation entries in annotations.json\")\n",
    "\n",
    "    # Loop through annotations and rename corresponding images\n",
    "    for idx, annotation in enumerate(annotations, 1):\n",
    "        old_name = annotation[\"OriginalFileName\"]\n",
    "        ext = os.path.splitext(old_name)[1].lower()  # Preserve the file extension\n",
    "        new_name = f\"{split}_original_{idx}{ext}\"\n",
    "\n",
    "        old_path = os.path.join(image_dir, old_name)\n",
    "        new_path = os.path.join(image_dir, new_name)\n",
    "\n",
    "        # Check if the image file exists before renaming\n",
    "        if os.path.exists(old_path):\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed {old_name} to {new_name}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Image file {old_name} not found in {image_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Update the annotation with the new image name\n",
    "        annotation[\"OriginalFileName\"] = new_name\n",
    "\n",
    "    # Save updated annotations\n",
    "    with open(annotation_path, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)\n",
    "\n",
    "    print(f\"{split} renamed and annotations updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9182d00",
   "metadata": {},
   "source": [
    "### Data loading & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8585276",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Defining a CustomDataset class ###\n",
    "######################################\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform = None):\n",
    "        \"\"\"\n",
    "        Initialize the custom dataset.\n",
    "        Works for both the train data and the test data.\n",
    "        \"\"\"\n",
    "        self.images_dir = os.path.join(data_path, \"images\")\n",
    "        self.transform = transform\n",
    "        annotations_file = data_path + \"/annotations.json\"\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            annotations_list = json.load(f)\n",
    "       \n",
    "        # We need to extract the bounding boxes of the annotations from the JSON file and store them as [x_min, y_min, x_max, y_max] tensors\n",
    "        self.data = []\n",
    "        for entry in annotations_list:\n",
    "            image_name = entry['OriginalFileName']\n",
    "            annotation_data = entry['AnnotationData']\n",
    "            bird_boxes = self.extract_bird_boxes(annotation_data)\n",
    "            self.data.append({'imagename': image_name, 'bird_boxes_tensor': bird_boxes})\n",
    "\n",
    "        # Note: we should not load all the images into a tensor here, as it would take too much memory. We load images into a tensor in the __getitem__ method.\n",
    "\n",
    "\n",
    "    def extract_bird_boxes(self, annotation_data):\n",
    "        \"\"\"\n",
    "        Extract the coordinates of the birds from the annotation data in the JSON file and return it as a tensor.\n",
    "        \"\"\"\n",
    "        bird_boxes = []\n",
    "        for entry in annotation_data:\n",
    "            if entry['Label'] == 'Bird':\n",
    "                coordinates_list = entry['Coordinates']\n",
    "                x_coordinates = [point['X'] for point in coordinates_list]\n",
    "                y_coordinates = [point['Y'] for point in coordinates_list]\n",
    "                x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "                y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "                bird_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return torch.tensor(bird_boxes, dtype=torch.float32) # Shape: (num_birds, 4)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset, i.e. the number of images.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load an image and its corresponding annotations.\n",
    "        Returns the image and a target dictionary with bounding boxes and labels (we need this for compatiblity with object detection models like Faster R-CNN)\n",
    "        \"\"\"\n",
    "        item = self.data[index]\n",
    "        image_path = os.path.join(self.images_dir, item['imagename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        bird_boxes = item['bird_boxes_tensor']\n",
    "        labels = torch.ones((bird_boxes.shape[0],), dtype=torch.int64) # Assuming all the labels are 'Bird' --> we assign this to class 1\n",
    "        target = {'boxes': bird_boxes, 'labels': labels} # should contain the bounding boxes and the labels\n",
    "\n",
    "        # Apply data augmentations\n",
    "        if self.transform is not None:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### Finding the mean and std of the dataset ###\n",
    "###############################################\n",
    "\n",
    "def calculate_mean_and_std(dataset):\n",
    "    # Initialize sums for mean and variance\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for image, _ in tqdm(dataset, desc=\"Calculating Mean and Std\", unit=\"image\"):\n",
    "        # Convert image to tensor if it is in PIL format\n",
    "        image = transforms.ToTensor()(image)  # shape: (C, H, W)\n",
    "        \n",
    "        # Calculate the sum and squared sum of pixels for each channel\n",
    "        mean += image.mean([1, 2])  # mean per channel (C,)\n",
    "        std += image.std([1, 2])    # std per channel (C,)\n",
    "        num_pixels += 1\n",
    "    \n",
    "    # Average the sums to get the mean and std\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# For now, do no transformations:\n",
    "train_data_original = CustomDataset(\"scarecrow_dataset/train\", transform=None)\n",
    "train_data_extra = CustomDataset(\"bird-detection-farm/train\", transform=None)\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "\n",
    "mean, std = calculate_mean_and_std(train_data)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165af7f",
   "metadata": {},
   "source": [
    "Dataset Mean: tensor([0.5390, 0.5306, 0.4421])\n",
    "\n",
    "Dataset Std: tensor([0.1624, 0.1527, 0.1647])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c97e9b",
   "metadata": {},
   "source": [
    "### With the extra data\n",
    "\n",
    "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
    "\n",
    "Dataset Std: tensor([0.1674, 0.1557, 0.1689])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Defining a CustomTransformation class ###\n",
    "##############################################\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "mean = [0.5409, 0.5505, 0.3894]\n",
    "std = [0.1674, 0.1557, 0.1689]\n",
    "\n",
    "class CustomTransformation:\n",
    "    def __init__(self):\n",
    "        self.transforms = transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean, std)])\n",
    "        \n",
    "\n",
    "    def perform_horizontal_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Horizontally flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.hflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            width, _ = image.size\n",
    "            x_min = boxes[:, 0].clone()\n",
    "            x_max = boxes[:, 2].clone()\n",
    "            boxes[:, 0] = width - x_max\n",
    "            boxes[:, 2] = width - x_min\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_vertical_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Vertically flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.vflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            _, height = image.size\n",
    "            y_min = boxes[:, 1].clone()\n",
    "            y_max = boxes[:, 3].clone()\n",
    "            boxes[:, 1] = height - y_max\n",
    "            boxes[:, 3] = height - y_min\n",
    "            target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def rotate_box(self, boxes, angle, img_width, img_height):\n",
    "        # Rotate in the opposite (clockwise) direction to match torchvision's CCW rotation\n",
    "        angle_rad = math.radians(-angle)\n",
    "\n",
    "        cx, cy = img_width / 2, img_height / 2\n",
    "\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            x0, y0, x1, y1 = box.tolist()\n",
    "            corners = [\n",
    "                [x0, y0],\n",
    "                [x1, y0],\n",
    "                [x1, y1],\n",
    "                [x0, y1]\n",
    "            ]\n",
    "            rotated = []\n",
    "            for x, y in corners:\n",
    "                # Translate to origin\n",
    "                x -= cx\n",
    "                y -= cy\n",
    "                # Rotate\n",
    "                x_new = x * math.cos(angle_rad) - y * math.sin(angle_rad)\n",
    "                y_new = x * math.sin(angle_rad) + y * math.cos(angle_rad)\n",
    "                # Translate back\n",
    "                x_new += cx\n",
    "                y_new += cy\n",
    "                rotated.append([x_new, y_new])\n",
    "            rotated = torch.tensor(rotated)\n",
    "            x_min, y_min = rotated.min(dim=0).values\n",
    "            x_max, y_max = rotated.max(dim=0).values\n",
    "            new_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        return torch.tensor(new_boxes)\n",
    "\n",
    "\n",
    "    def perform_random_rotation(self, image, target, prob=0.25, rotations=[90, 180, 270]):\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice(rotations)\n",
    "            w, h = image.size\n",
    "            image = TF.rotate(image, angle)  # CCW rotation\n",
    "            boxes = target['boxes']\n",
    "            target['boxes'] = self.rotate_box(boxes, angle, w, h)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "    def perform_random_resize(self, image, target, scale_range=(0.75, 1.25)):\n",
    "        \"\"\"\n",
    "        Perform a random reize within the specified scale range, default scale range is (0,75, 1.25)\n",
    "        \"\"\"\n",
    "        scale = random.uniform(*scale_range)\n",
    "\n",
    "        # Resize the image\n",
    "        width, height = image.size\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        image = functional.resize(image, [new_height, new_width])\n",
    "        \n",
    "        # Resize the boxes\n",
    "        boxes = target['boxes']\n",
    "        boxes = boxes * scale\n",
    "        target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        Apply the transformations to an image\n",
    "        \"\"\"\n",
    "        image, target = self.perform_horizontal_flip(image, target)\n",
    "        image, target = self.perform_vertical_flip(image, target)\n",
    "        image, target = self.perform_random_rotation(image, target)\n",
    "        image, target = self.perform_random_resize(image, target)\n",
    "        image = self.transforms(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Define paths\n",
    "train_data_original_path = \"scarecrow_dataset/train\"\n",
    "test_data_original_path = \"scarecrow_dataset/test\"\n",
    "val_data_original_path = \"scarecrow_dataset/val\"\n",
    "train_data_extra_path = \"bird-detection-farm/train\"\n",
    "valid_data_extra_path = \"bird-detection-farm/valid\"\n",
    "test_data_extra_path = \"bird-detection-farm/test\"\n",
    "\n",
    "# Loading the datasets with the transformations\n",
    "transform = CustomTransformation()\n",
    "\n",
    "train_data_raw = CustomDataset(train_data_original_path)\n",
    "train_data_original = CustomDataset(train_data_original_path, transform)\n",
    "valid_data_original = CustomDataset(val_data_original_path, transform)\n",
    "test_data_original = CustomDataset(test_data_original_path, transform)\n",
    "\n",
    "train_data_extra = CustomDataset(train_data_extra_path, transform)\n",
    "valid_data_extra = CustomDataset(valid_data_extra_path, transform)\n",
    "test_data_extra = CustomDataset(test_data_extra_path, transform)\n",
    "\n",
    "# Split old training set into train/val\n",
    "#train_data_original, valid_data_original = torch.utils.data.random_split(train_data_original, [0.8, 0.2])\n",
    "\n",
    "# Combine datasets\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "valid_data = torch.utils.data.ConcatDataset([valid_data_original, valid_data_extra])\n",
    "test_data = torch.utils.data.ConcatDataset([test_data_original, test_data_extra])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# No shuffling for validation and test data because we want consistnt order for reproducibility:\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one image and its corresponding target from the dataset\n",
    "image, target = train_data_original[88]  # Replace 0 with any index to fetch a different image\n",
    "\n",
    "# Convert the image tensor to a NumPy array for visualization\n",
    "image_np = image.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "image_np = (image_np * std + mean).clip(0, 1)  # Denormalize the image\n",
    "\n",
    "# Visualize the image with bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image_np)\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in target['boxes']:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    rect = patches.Rectangle(\n",
    "        (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "        linewidth=2, edgecolor='r', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ea951",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### Debugging data augmentation ###\n",
    "###################################\n",
    "\n",
    "# import copy\n",
    "# import numpy as np\n",
    "\n",
    "# random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Choose the index of an image to test\n",
    "# idx = 55\n",
    "# image_raw, target_raw = train_data_raw[idx]\n",
    "\n",
    "# def visualize(image, boxes, title=\"Image\"):\n",
    "#     # Convert PIL image to NumPy\n",
    "#     if isinstance(image, torch.Tensor):\n",
    "#         image = image.permute(1, 2, 0).numpy()\n",
    "#     elif hasattr(image, 'size') and not isinstance(image, np.ndarray):\n",
    "#         # It's a PIL image — convert to NumPy and scale to [0, 1]\n",
    "#         image = np.array(image).astype('float32') / 255.0\n",
    "\n",
    "#     image = np.clip(image, 0, 1)\n",
    "\n",
    "#     fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "#     ax.imshow(image)\n",
    "\n",
    "#     for box in boxes:\n",
    "#         x_min, y_min, x_max, y_max = box\n",
    "#         rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "#                                  linewidth=2, edgecolor='red', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# transform = CustomTransformation()\n",
    "\n",
    "# print(image_raw, target_raw)\n",
    "\n",
    "# # Test individual transformations manually\n",
    "# image_aug, target_aug = transform.perform_horizontal_flip(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0\n",
    "# )\n",
    "\n",
    "# image_aug3, target_aug3 = transform.perform_vertical_flip(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0\n",
    "# )\n",
    "\n",
    "# image_aug2, target_aug2 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0\n",
    "# )\n",
    "\n",
    "# image_aug4, target_aug4 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0, rotations = [90]\n",
    "# )\n",
    "\n",
    "# image_aug5, target_aug5 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0, rotations = [180]\n",
    "# )\n",
    "\n",
    "# image_aug6, target_aug6 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0, rotations = [270])\n",
    "\n",
    "# # Visualize before/after\n",
    "# visualize(image_raw, target_raw['boxes'], title=\"Original\")\n",
    "# visualize(image_aug, target_aug['boxes'], title=\"Horizontally Flipped\")\n",
    "# visualize(image_aug3, target_aug3['boxes'], title=\"Vertically Flipped\")\n",
    "# visualize(image_aug4, target_aug4['boxes'], title=\"90 degree rotation\")\n",
    "# visualize(image_aug5, target_aug5['boxes'], title=\"180 degree rotation\")\n",
    "# visualize(image_aug6, target_aug6['boxes'], title=\"270 degree rotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8eb2fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created txt for:Schermafbeelding 2024-06-04 104831.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104857.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104648.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105612.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105136.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105402.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105341.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104959.png\n",
      "Created txt for:20240903085339_0270_D_frame_1890 - kopie.png\n",
      "Created txt for:20240903090245_0271_D_frame_960 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3780 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_3960 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_1110 - kopie.png\n",
      "Created txt for:20240901115456_0265_D_frame_1200 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_4440 - kopie.png\n",
      "Created txt for:20240904110435_0274_D_frame_2100.png\n",
      "Created txt for:20240906133805_0281_D_frame_900.png\n",
      "Created txt for:20240906133238_0280_D_frame_4320.png\n",
      "Created txt for:20240904083832_0272_D_frame_6000.png\n",
      "Created txt for:20240904083832_0272_D_frame_3840.png\n",
      "Created txt for:20240906133238_0280_D_frame_3960.png\n",
      "Created txt for:20240906133805_0281_D_frame_240.png\n",
      "Created txt for:20240904083832_0272_D_frame_4320.png\n",
      "Created txt for:DJI_0331.JPG\n",
      "Created txt for:DJI_0332.JPG\n",
      "Created txt for:DJI_0319.JPG\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m37s625.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m20s957.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h49m00s549.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m05s670.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m08s654.png\n",
      "Created txt for:DJI_0358_frame_360.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105530.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102707.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102122.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105217.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104532.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102227.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102744.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104541.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101800.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102731.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105154.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105244.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101719.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101700.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102006.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102504.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102404.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102439.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101952.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104613.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102556.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104738.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105259.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105010.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104932.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104504.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104559.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102348.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102419.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101908.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102300.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102634.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104453.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102135.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101731.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105202.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105121.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102100.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102315.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105327.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102246.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 101823.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104713.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104704.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102034.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104521.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102620.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105309.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102210.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104909.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104923.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 102529.png\n",
      "Created txt for:20240903084813_0269_D_frame_2880 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_540 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_8760 - kopie.png\n",
      "Created txt for:20240903090245_0271_D_frame_1410 - kopie.png\n",
      "Created txt for:20240901115456_0265_D_frame_4590 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_1470 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_420 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_1170 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3840 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_2310 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4020 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_420 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_9600 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2820 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_780 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2400 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_4020 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_2280 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2520 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4950 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_1500 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3570 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3600 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3210 - kopie.png\n",
      "Created txt for:20240901115456_0265_D_frame_4560 - kopie.png\n",
      "Created txt for:20240903090245_0271_D_frame_1440 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_8100 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4290 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2280 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2040 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_5520 - kopie.png\n",
      "Created txt for:20240901120230_0267_D_frame_630 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_60 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_750 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4410 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_5160 - kopie.png\n",
      "Created txt for:20240901115456_0265_D_frame_1170 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2760 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4350 - kopie.png\n",
      "Created txt for:20240903090245_0271_D_frame_930 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_930 - kopie.png\n",
      "Created txt for:20240903090245_0271_D_frame_900 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4470 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_1380 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_2130 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_5460 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2940 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3120 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4380 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_2460 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_1200 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_9480 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4110 - kopie.png\n",
      "Created txt for:20240901115456_0265_D_frame_1230 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_9720 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3090 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_1920 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_8880 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3540 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_1620 - kopie.png\n",
      "Created txt for:20240906133805_0281_D_frame_2160.png\n",
      "Created txt for:20240904083832_0272_D_frame_840.png\n",
      "Created txt for:20240903171845_0037_D_frame_3180.png\n",
      "Created txt for:20240906133805_0281_D_frame_1980.png\n",
      "Created txt for:20240906133805_0281_D_frame_1800.png\n",
      "Created txt for:20240903171845_0037_D_frame_720.png\n",
      "Created txt for:20240904110435_0274_D_frame_1140.png\n",
      "Created txt for:20240904083832_0272_D_frame_6720.png\n",
      "Created txt for:20240904110435_0274_D_frame_1020.png\n",
      "Created txt for:20240904083832_0272_D_frame_6600.png\n",
      "Created txt for:20240906133238_0280_D_frame_6960.png\n",
      "Created txt for:20240906133805_0281_D_frame_3180.png\n",
      "Created txt for:20240906133805_0281_D_frame_540.png\n",
      "Created txt for:20240904083832_0272_D_frame_2880.png\n",
      "Created txt for:20240903171845_0037_D_frame_1200.png\n",
      "Created txt for:20240904110435_0274_D_frame_780.png\n",
      "Created txt for:20240906133238_0280_D_frame_5640.png\n",
      "Created txt for:20240904083832_0272_D_frame_3000.png\n",
      "Created txt for:20240903171845_0037_D_frame_1080.png\n",
      "Created txt for:20240906133805_0281_D_frame_1260.png\n",
      "Created txt for:20240906133805_0281_D_frame_1500.png\n",
      "Created txt for:20240904110435_0274_D_frame_2940.png\n",
      "Created txt for:20240906133805_0281_D_frame_3900.png\n",
      "Created txt for:20240904083832_0272_D_frame_4200.png\n",
      "Created txt for:20240903171845_0037_D_frame_780.png\n",
      "Created txt for:20240903171845_0037_D_frame_2940.png\n",
      "Created txt for:20240903171845_0037_D_frame_900.png\n",
      "Created txt for:20240906133238_0280_D_frame_3480.png\n",
      "Created txt for:20240904110435_0274_D_frame_720.png\n",
      "Created txt for:20240904083832_0272_D_frame_4560.png\n",
      "Created txt for:20240906133238_0280_D_frame_4920.png\n",
      "Created txt for:20240904083832_0272_D_frame_6960.png\n",
      "Created txt for:20240904110435_0274_D_frame_120.png\n",
      "Created txt for:20240906133805_0281_D_frame_60.png\n",
      "Created txt for:20240906133238_0280_D_frame_7200.png\n",
      "Created txt for:20240904110435_0274_D_frame_2160.png\n",
      "Created txt for:20240906133805_0281_D_frame_1320.png\n",
      "Created txt for:20240904110435_0274_D_frame_960.png\n",
      "Created txt for:20240904110435_0274_D_frame_1260.png\n",
      "Created txt for:20240906133238_0280_D_frame_4080.png\n",
      "Created txt for:20240903171845_0037_D_frame_1020.png\n",
      "Created txt for:20240903171845_0037_D_frame_2400.png\n",
      "Created txt for:20240903171845_0037_D_frame_3420.png\n",
      "Created txt for:20240903171845_0037_D_frame_960.png\n",
      "Created txt for:20240906133805_0281_D_frame_120.png\n",
      "Created txt for:20240906133805_0281_D_frame_840.png\n",
      "Created txt for:20240904084357_0273_D_frame_240.png\n",
      "Created txt for:20240903171845_0037_D_frame_840.png\n",
      "Created txt for:20240904083832_0272_D_frame_2760.png\n",
      "Created txt for:20240906133238_0280_D_frame_3600.png\n",
      "Created txt for:20240906133238_0280_D_frame_6120.png\n",
      "Created txt for:20240906133805_0281_D_frame_3780.png\n",
      "Created txt for:20240904083832_0272_D_frame_1320.png\n",
      "Created txt for:20240906133238_0280_D_frame_4440.png\n",
      "Created txt for:20240906133238_0280_D_frame_3720.png\n",
      "Created txt for:20240904083832_0272_D_frame_5880.png\n",
      "Created txt for:20240904110435_0274_D_frame_0.png\n",
      "Created txt for:20240903171845_0037_D_frame_3060.png\n",
      "Created txt for:20240904083832_0272_D_frame_6240.png\n",
      "Created txt for:20240904110435_0274_D_frame_2220.png\n",
      "Created txt for:20240904083832_0272_D_frame_5640.png\n",
      "Created txt for:20240906133805_0281_D_frame_3960.png\n",
      "Created txt for:20240906133805_0281_D_frame_4140.png\n",
      "Created txt for:20240904083832_0272_D_frame_1080.png\n",
      "Created txt for:20240903171845_0037_D_frame_3300.png\n",
      "Created txt for:20240904083832_0272_D_frame_3360.png\n",
      "Created txt for:20240906133238_0280_D_frame_6480.png\n",
      "Created txt for:20240906133238_0280_D_frame_7800.png\n",
      "Created txt for:20240906133238_0280_D_frame_840.png\n",
      "Created txt for:DJI_0322.JPG\n",
      "Created txt for:DJI_0316.JPG\n",
      "Created txt for:DJI_0320.JPG\n",
      "Created txt for:DJI_0318.JPG\n",
      "Created txt for:DJI_0216_frame_300.png\n",
      "Created txt for:DJI_0186.JPG\n",
      "Created txt for:DJI_0315.JPG\n",
      "Created txt for:DJI_0216_frame_180.png\n",
      "Created txt for:DJI_0312.JPG\n",
      "Created txt for:DJI_0317.JPG\n",
      "Created txt for:DJI_0311.JPG\n",
      "Created txt for:DJI_0329.JPG\n",
      "Created txt for:DJI_0101.JPG\n",
      "Created txt for:DJI_0330.JPG\n",
      "Created txt for:DJI_0336.JPG\n",
      "Created txt for:DJI_0324.JPG\n",
      "Created txt for:DJI_0216_frame_240.png\n",
      "Created txt for:DJI_0310.JPG\n",
      "Created txt for:DJI_0345.JPG\n",
      "Created txt for:DJI_0313.JPG\n",
      "Created txt for:DJI_0335.JPG\n",
      "Created txt for:DJI_0333.JPG\n",
      "Created txt for:DJI_0216_frame_120.png\n",
      "Created txt for:DJI_0334.JPG\n",
      "Created txt for:DJI_0164.JPG\n",
      "Created txt for:DJI_0321.JPG\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m36s811.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m31s299.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m18s782.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m32s623.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m13s782.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h49m58s948.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h52m53s317.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m26s591.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m11s737.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m32s948.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m35s427.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h58m32s761.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h58m01s594.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m01s381.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m32s418.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m40s634.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m04s332.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m19s743.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m47s659.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h58m41s474.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m25s573.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m06s399.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m08s727.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h52m31s064.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h52m35s182.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h58m05s793.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h48m59s570.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m03s318.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m16s733.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m20s772.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m22s031.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h55m47s464.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m22s161.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h52m58s755.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m14s934.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h55m42s638.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h52m45s763.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h55m45s875.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m05s685.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m46s339.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m34s193.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h55m12s636.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m30s608.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m05s261.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h54m18s074.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h50m22s485.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m55s943.png\n",
      "Created txt for:DJI_0358_frame_450.png\n",
      "Created txt for:DJI_0358_frame_1050.png\n",
      "Created txt for:DJI_0358_frame_480.png\n",
      "Created txt for:DJI_0358_frame_1830.png\n",
      "Created txt for:DJI_0358_frame_330.png\n",
      "Created txt for:DJI_0358_frame_1800.png\n",
      "Created txt for:DJI_0358_frame_390.png\n",
      "Created txt for:DJI_0358_frame_420.png\n",
      "Created txt for:DJI_0358_frame_300.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105113.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104630.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104842.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105353.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104944.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 105432.png\n",
      "Created txt for:Schermafbeelding 2024-06-04 104818.png\n",
      "Created txt for:20240901120856_0268_D_frame_1020 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_3000 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_3660 - kopie.png\n",
      "Created txt for:20240901120856_0268_D_frame_1080 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_1440 - kopie.png\n",
      "Created txt for:20240903085339_0270_D_frame_4050 - kopie.png\n",
      "Created txt for:20240903084813_0269_D_frame_1740 - kopie.png\n",
      "Created txt for:20240906133805_0281_D_frame_3720.png\n",
      "Created txt for:20240903171845_0037_D_frame_660.png\n",
      "Created txt for:20240904083832_0272_D_frame_3720.png\n",
      "Created txt for:20240903171845_0037_D_frame_3660.png\n",
      "Created txt for:20240903171845_0037_D_frame_2760.png\n",
      "Created txt for:20240904110435_0274_D_frame_1800.png\n",
      "Created txt for:20240906133238_0280_D_frame_4200.png\n",
      "Created txt for:20240903171845_0037_D_frame_3120.png\n",
      "Created txt for:DJI_0184.JPG\n",
      "Created txt for:DJI_0314.JPG\n",
      "Created txt for:DJI_0323.JPG\n",
      "Created txt for:vlcsnap-2024-11-15-11h59m39s766.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h57m55s121.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h52m39s652.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h58m07s929.png\n",
      "Created txt for:vlcsnap-2024-11-15-11h56m23s608.png\n",
      "Created txt for:DJI_0358_frame_1020.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "#this changes the annations format to Txt which Yolo can read. \n",
    "def convert_to_yolo_format(data_path, annotations_file, output_dir):\n",
    "    # Load annotations JSON file\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations_list = json.load(f)\n",
    "\n",
    "    # Ensure output directories exist\n",
    "        os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "\n",
    "    #dit moest blijkbaar\n",
    "    class_map = {\"Bird\": 0}\n",
    "\n",
    "    # ocess each image\n",
    "    for entry in annotations_list:\n",
    "        image_name = entry['OriginalFileName']\n",
    "        annotation_data = entry['AnnotationData']\n",
    "\n",
    "\n",
    "        # Load image to get width and height\n",
    "        image_path = os.path.join(data_path, 'images', image_name)\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            #print(f\"Image size: {img_width} x {img_height}\")\n",
    "\n",
    "        # Create the label file for this image\n",
    "        label_file = os.path.join(output_dir, 'labels', os.path.splitext(image_name)[0] + '.txt')\n",
    "\n",
    "        with open(label_file, 'w') as label_f:\n",
    "            for obj in annotation_data:\n",
    "                class_name = obj['Label']  # 'Label' field in your data\n",
    "                \n",
    "                if class_name in class_map:\n",
    "                    # Get the coordinates (bounding box)\n",
    "                    coordinates = obj['Coordinates']\n",
    "                    \n",
    "                    # Calculate bounding box (x_min, y_min, width, height)\n",
    "                    x_min = min([coord['X'] for coord in coordinates])\n",
    "                    y_min = min([coord['Y'] for coord in coordinates])\n",
    "                    x_max = max([coord['X'] for coord in coordinates])\n",
    "                    y_max = max([coord['Y'] for coord in coordinates])\n",
    "\n",
    "                    # YOLO format: class_id x_center y_center width height (all normalized)\n",
    "                    class_id = class_map[class_name]\n",
    "                    x_center = (x_min + x_max) / 2 / img_width\n",
    "                    y_center = (y_min + y_max) / 2 / img_height\n",
    "                    norm_width = (x_max - x_min) / img_width\n",
    "                    norm_height = (y_max - y_min) / img_height\n",
    "                    \n",
    "                    # Write the YOLO annotation to the label file\n",
    "                    label_f.write(f\"{class_id} {x_center} {y_center} {norm_width} {norm_height}\\n\")\n",
    "                    \n",
    "           \n",
    "        print(\"Created txt for:\"+image_name)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "data_path = \"scarecrow_dataset/val\"  # Path to the train folder\n",
    "annotations_file = \"scarecrow_dataset/val/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"scarecrow_dataset/val\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n",
    "data_path = \"scarecrow_dataset/train\"  # Path to the train folder\n",
    "annotations_file = \"scarecrow_dataset/train/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"scarecrow_dataset/train\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n",
    "data_path = \"scarecrow_dataset/test\"  # Path to the train folder\n",
    "annotations_file = \"scarecrow_dataset/test/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"scarecrow_dataset/test\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d90992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.128  Python-3.12.7 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i7-1195G7 2.90GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=c:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\runs\\detect\\train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'data.yaml' error  Dataset 'data.yaml' images not found, missing path 'C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\datasets\\DL_angrybirds\\scarecrow_dataset\\val\\images'\nNote dataset download directory is 'C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\datasets'. You can update this in 'C:\\Users\\thomz\\AppData\\Roaming\\Ultralytics\\settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:583\u001b[0m, in \u001b[0;36mBaseTrainer.get_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    582\u001b[0m }:\n\u001b[1;32m--> 583\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\data\\utils.py:454\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[1;34m(dataset, autodownload)\u001b[0m\n\u001b[0;32m    453\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote dataset download directory is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can update this in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[0;32m    455\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Dataset 'data.yaml' images not found, missing path 'C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\datasets\\DL_angrybirds\\scarecrow_dataset\\val\\images'\nNote dataset download directory is 'C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\datasets'. You can update this in 'C:\\Users\\thomz\\AppData\\Roaming\\Ultralytics\\settings.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Load YOLO model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Example: Add dropout to YOLO layers\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Path to dataset YAML file\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# Set to 0 for GPU, 'cpu' for CPU\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the validation set\u001b[39;00m\n\u001b[0;32m     15\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:787\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    785\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[1;32m--> 787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:139\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:587\u001b[0m, in \u001b[0;36mBaseTrainer.get_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m error ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msingle_cls:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset 'data.yaml' error  Dataset 'data.yaml' images not found, missing path 'C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\datasets\\DL_angrybirds\\scarecrow_dataset\\val\\images'\nNote dataset download directory is 'C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\datasets'. You can update this in 'C:\\Users\\thomz\\AppData\\Roaming\\Ultralytics\\settings.json'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")  # Load YOLO model\n",
    "# Example: Add dropout to YOLO layers\n",
    "\n",
    "model.train(\n",
    "    data='data.yaml', # Path to dataset YAML file\n",
    "    epochs=50,                          \n",
    "    imgsz=640,                          \n",
    "    batch=32,                           \n",
    "    device='cpu'                             # Set to 0 for GPU, 'cpu' for CPU\n",
    ")\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(\"scarecrow_dataset/test/images/20240901120856_0268_D_frame_1020 - kopie.png\")  # Predict on an image\n",
    "results[0].show()  # Display results\n",
    "#model.save('yolov8_trained.pt') # saved yolov8s model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This checks all the hyperparameters and finds the best one\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "batch_sizes = [32]\n",
    "optimizers = ['SGD', 'Adam']\n",
    "momentums = [0.9, 0.99]  # Only used for SGD\n",
    "weight_decays = [1e-6, 1e-5, 1e-4]\n",
    "image_sizes = [480, 640]\n",
    "\n",
    "# Initialize YOLO model\n",
    "model_path = 'yolo11n.pt'\n",
    "data_path = './data.yaml'\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "best_map = 0\n",
    "best_params = None\n",
    "\n",
    "for lr, batch_size, optimizer, img_size, weight_decay in itertools.product(\n",
    "    learning_rates, batch_sizes, optimizers, image_sizes, weight_decays\n",
    "):\n",
    "    # Set momentum only for SGD\n",
    "    momentum = 0.9 if optimizer == 'SGD' else None\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        epochs=15,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        device=0,\n",
    "        lr0=lr,\n",
    "        optimizer=optimizer,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Get the metric to optimize (e.g., mAP@50)\n",
    "    map50 = results.maps\n",
    "\n",
    "    # Log the results\n",
    "    print(f\"lr: {lr}, batch_size: {batch_size}, optimizer: {optimizer}, img_size: {img_size}, weight_decay: {weight_decay}, mAP@50: {map50}\")\n",
    "\n",
    "    # Update the best parameters\n",
    "    if map50 > best_map:\n",
    "        best_map = map50\n",
    "        best_params = {\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'optimizer': optimizer,\n",
    "            'momentum': momentum,\n",
    "            'weight_decay': weight_decay,\n",
    "            'img_size': img_size\n",
    "        }\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best mAP@50:\", best_map)\n",
    "#Application of regularization techniques (e.g., dropout, batch normalization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is there to check if the Annotations are working correctly.\n",
    "def visualize_yolo_annotations(image_path, label_path, class_names=None):\n",
    " \n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_width, img_height = image.size\n",
    "\n",
    "    # Create a plot\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Read the YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse each line in the label file\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])\n",
    "        x_center, y_center, width, height = map(float, parts[1:])\n",
    "\n",
    "        # Convert normalized coordinates to absolute pixel values\n",
    "        x_center *= img_width\n",
    "        y_center *= img_height\n",
    "        width *= img_width\n",
    "        height *= img_height\n",
    "\n",
    "        # Calculate the top-left corner of the bounding box\n",
    "        x_min = x_center - (width / 2)\n",
    "        y_min = y_center - (height / 2)\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min), width, height,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label (if provided)\n",
    "        if class_names:\n",
    "            ax.text(\n",
    "                x_min, y_min - 5, class_names[class_id],\n",
    "                color='red', fontsize=12, backgroundcolor='white'\n",
    "            )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = \"scarecrow_dataset/train/images/train_original_89.png\"\n",
    "label_path = \"scarecrow_dataset/train/labels/train_original_89.txt\"\n",
    "class_names = [\"Bird\"]  #\n",
    "\n",
    "visualize_yolo_annotations(image_path, label_path, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
