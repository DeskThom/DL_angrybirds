{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc615d47",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b751e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.transforms import functional\n",
    "from torch.utils.data import ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import itertools\n",
    "# from ultralytics import YOLO\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, device_name = (torch.device(\"cuda\"), torch.cuda.get_device_name(0)) if torch.cuda.is_available() else (torch.device(\"cpu\"), \"CPU\")\n",
    "print(f\"Device: {device}, {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9182d00",
   "metadata": {},
   "source": [
    "### Data loading & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8585276",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Defining a CustomDataset class ###\n",
    "######################################\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform = None):\n",
    "        \"\"\"\n",
    "        Initialize the custom dataset.\n",
    "        Works for both the train data and the test data.\n",
    "        \"\"\"\n",
    "        self.images_dir = os.path.join(data_path, \"images\")\n",
    "        self.transform = transform\n",
    "        annotations_file = data_path + \"/annotations.json\"\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            annotations_list = json.load(f)\n",
    "       \n",
    "        # We need to extract the bounding boxes of the annotations from the JSON file and store them as [x_min, y_min, x_max, y_max] tensors\n",
    "        self.data = []\n",
    "        for entry in annotations_list:\n",
    "            image_name = entry['OriginalFileName']\n",
    "            annotation_data = entry['AnnotationData']\n",
    "            bird_boxes = self.extract_bird_boxes(annotation_data)\n",
    "            self.data.append({'imagename': image_name, 'bird_boxes_tensor': bird_boxes})\n",
    "\n",
    "        # Note: we should not load all the images into a tensor here, as it would take too much memory. We load images into a tensor in the __getitem__ method.\n",
    "\n",
    "\n",
    "    def extract_bird_boxes(self, annotation_data):\n",
    "        \"\"\"\n",
    "        Extract the coordinates of the birds from the annotation data in the JSON file and return it as a tensor.\n",
    "        \"\"\"\n",
    "        bird_boxes = []\n",
    "        for entry in annotation_data:\n",
    "            if entry['Label'] == 'Bird':\n",
    "                coordinates_list = entry['Coordinates']\n",
    "                x_coordinates = [point['X'] for point in coordinates_list]\n",
    "                y_coordinates = [point['Y'] for point in coordinates_list]\n",
    "                x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "                y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "                bird_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return torch.tensor(bird_boxes, dtype=torch.float32) # Shape: (num_birds, 4)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset, i.e. the number of images.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load an image and its corresponding annotations.\n",
    "        Returns the image and a target dictionary with bounding boxes and labels (we need this for compatiblity with object detection models like Faster R-CNN)\n",
    "        \"\"\"\n",
    "        item = self.data[index]\n",
    "        image_path = os.path.join(self.images_dir, item['imagename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        bird_boxes = item['bird_boxes_tensor']\n",
    "        labels = torch.ones((bird_boxes.shape[0],), dtype=torch.int64) # Assuming all the labels are 'Bird' --> we assign this to class 1\n",
    "        target = {'boxes': bird_boxes, 'labels': labels} # should contain the bounding boxes and the labels\n",
    "\n",
    "        # Apply data augmentations\n",
    "        if self.transform is not None:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480ec51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Mean and Std: 100%|██████████| 381/381 [01:55<00:00,  3.31image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
      "Dataset Std: tensor([0.1674, 0.1557, 0.1689])\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "### Finding the mean and std of the dataset ###\n",
    "###############################################\n",
    "\n",
    "def calculate_mean_and_std(dataset):\n",
    "    # Initialize sums for mean and variance\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for image, _ in tqdm(dataset, desc=\"Calculating Mean and Std\", unit=\"image\"):\n",
    "        # Convert image to tensor if it is in PIL format\n",
    "        image = transforms.ToTensor()(image)  # shape: (C, H, W)\n",
    "        \n",
    "        # Calculate the sum and squared sum of pixels for each channel\n",
    "        mean += image.mean([1, 2])  # mean per channel (C,)\n",
    "        std += image.std([1, 2])    # std per channel (C,)\n",
    "        num_pixels += 1\n",
    "    \n",
    "    # Average the sums to get the mean and std\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# For now, do no transformations:\n",
    "train_data_original = CustomDataset(\"datasets/scarecrow_dataset/train\", transform=None)\n",
    "train_data_extra = CustomDataset(\"datasets/bird-detection-farm/train\", transform=None)\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "\n",
    "mean, std = calculate_mean_and_std(train_data)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165af7f",
   "metadata": {},
   "source": [
    "Dataset Mean: tensor([0.5390, 0.5306, 0.4421])\n",
    "\n",
    "Dataset Std: tensor([0.1624, 0.1527, 0.1647])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c97e9b",
   "metadata": {},
   "source": [
    "### With the extra data\n",
    "\n",
    "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
    "\n",
    "Dataset Std: tensor([0.1674, 0.1557, 0.1689])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c519cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Defining a CustomTransformation class ###\n",
    "##############################################\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "mean = [0.5409, 0.5505, 0.3894]\n",
    "std = [0.1674, 0.1557, 0.1689]\n",
    "\n",
    "class CustomTransformation:\n",
    "    def __init__(self):\n",
    "        self.transforms = transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean, std)])\n",
    "        \n",
    "\n",
    "    def perform_horizontal_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Horizontally flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.hflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            width, _ = image.size\n",
    "            x_min = boxes[:, 0].clone()\n",
    "            x_max = boxes[:, 2].clone()\n",
    "            boxes[:, 0] = width - x_max\n",
    "            boxes[:, 2] = width - x_min\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_vertical_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Vertically flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.vflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            _, height = image.size\n",
    "            y_min = boxes[:, 1].clone()\n",
    "            y_max = boxes[:, 3].clone()\n",
    "            boxes[:, 1] = height - y_max\n",
    "            boxes[:, 3] = height - y_min\n",
    "            target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def rotate_box(self, boxes, angle, img_width, img_height):\n",
    "        # Rotate in the opposite (clockwise) direction to match torchvision's CCW rotation\n",
    "        angle_rad = math.radians(-angle)\n",
    "\n",
    "        cx, cy = img_width / 2, img_height / 2\n",
    "\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            x0, y0, x1, y1 = box.tolist()\n",
    "            corners = [\n",
    "                [x0, y0],\n",
    "                [x1, y0],\n",
    "                [x1, y1],\n",
    "                [x0, y1]\n",
    "            ]\n",
    "            rotated = []\n",
    "            for x, y in corners:\n",
    "                # Translate to origin\n",
    "                x -= cx\n",
    "                y -= cy\n",
    "                # Rotate\n",
    "                x_new = x * math.cos(angle_rad) - y * math.sin(angle_rad)\n",
    "                y_new = x * math.sin(angle_rad) + y * math.cos(angle_rad)\n",
    "                # Translate back\n",
    "                x_new += cx\n",
    "                y_new += cy\n",
    "                rotated.append([x_new, y_new])\n",
    "            rotated = torch.tensor(rotated)\n",
    "            x_min, y_min = rotated.min(dim=0).values\n",
    "            x_max, y_max = rotated.max(dim=0).values\n",
    "            new_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        return torch.tensor(new_boxes)\n",
    "\n",
    "\n",
    "    def perform_random_rotation(self, image, target, prob=0.25, rotations=[90, 180, 270]):\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice(rotations)\n",
    "            w, h = image.size\n",
    "            image = TF.rotate(image, angle)  # CCW rotation\n",
    "            boxes = target['boxes']\n",
    "            target['boxes'] = self.rotate_box(boxes, angle, w, h)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "    def perform_random_resize(self, image, target, scale_range=(0.75, 1.25)):\n",
    "        \"\"\"\n",
    "        Perform a random reize within the specified scale range, default scale range is (0,75, 1.25)\n",
    "        \"\"\"\n",
    "        scale = random.uniform(*scale_range)\n",
    "\n",
    "        # Resize the image\n",
    "        width, height = image.size\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        image = functional.resize(image, [new_height, new_width])\n",
    "        \n",
    "        # Resize the boxes\n",
    "        boxes = target['boxes']\n",
    "        boxes = boxes * scale\n",
    "        target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        Apply the transformations to an image\n",
    "        \"\"\"\n",
    "        image, target = self.perform_horizontal_flip(image, target)\n",
    "        image, target = self.perform_vertical_flip(image, target)\n",
    "        image, target = self.perform_random_rotation(image, target)\n",
    "        image, target = self.perform_random_resize(image, target)\n",
    "        image = self.transforms(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd78746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Define paths\n",
    "train_data_original_path = \"datasets/scarecrow_dataset/train\"\n",
    "test_data_original_path = \"datasets/scarecrow_dataset/test\"\n",
    "val_data_original_path = \"datasets/scarecrow_dataset/val\"\n",
    "train_data_extra_path = \"datasets/bird-detection-farm/train\"\n",
    "valid_data_extra_path = \"datasets/bird-detection-farm/valid\"\n",
    "test_data_extra_path = \"datasets/bird-detection-farm/test\"\n",
    "\n",
    "# Loading the datasets with the transformations\n",
    "transform = CustomTransformation()\n",
    "\n",
    "train_data_raw = CustomDataset(train_data_original_path)\n",
    "train_data_original = CustomDataset(train_data_original_path, transform)\n",
    "valid_data_original = CustomDataset(val_data_original_path, transform)\n",
    "test_data_original = CustomDataset(test_data_original_path, transform)\n",
    "\n",
    "train_data_extra = CustomDataset(train_data_extra_path, transform)\n",
    "valid_data_extra = CustomDataset(valid_data_extra_path, transform)\n",
    "test_data_extra = CustomDataset(test_data_extra_path, transform)\n",
    "\n",
    "# Split old training set into train/val\n",
    "#train_data_original, valid_data_original = torch.utils.data.random_split(train_data_original, [0.8, 0.2])\n",
    "\n",
    "# Combine datasets\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "valid_data = torch.utils.data.ConcatDataset([valid_data_original, valid_data_extra])\n",
    "test_data = torch.utils.data.ConcatDataset([test_data_original, test_data_extra])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# No shuffling for validation and test data because we want consistnt order for reproducibility:\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
