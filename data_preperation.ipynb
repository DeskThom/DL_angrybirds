{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f9751c",
   "metadata": {},
   "source": [
    "# Deep Learning group assignment\n",
    "Group name: Angry Birds\n",
    "\n",
    "Group members:\n",
    "- Nienke Reijnen: 2117034\n",
    "- Andrea Ciavatti: 2115635\n",
    "- Niels Boonstra: 1451294\n",
    "- Yannick Lankhorst: 2052754\n",
    "- Thom Zoomer:2059225\n",
    "- Anne Barnasconi: 2053988"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc615d47",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1df5d",
   "metadata": {},
   "source": [
    "Before running, make sure to also have installed the following packages (according to lab 8 instructions):\n",
    "- pip install imageio\n",
    "- pip install future\n",
    "- pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b751e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.transforms import functional\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, device_name = (torch.device(\"cuda\"), torch.cuda.get_device_name(0)) if torch.cuda.is_available() else (torch.device(\"cpu\"), \"CPU\")\n",
    "print(f\"Device: {device}, {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f9ca2",
   "metadata": {},
   "source": [
    "## Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0c073",
   "metadata": {},
   "source": [
    "- DONE Correct implementation of data loaders for images and annotations for your specific object detection model\n",
    "- DONE Use of data augmentation techniques\n",
    "- DONE Appropriate shuffling  and batching of data\n",
    "- TO DO: Conduct an online search for relevant open-source datasets, and if you can find them, use them in your application as additional training data (to improve generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999552a",
   "metadata": {},
   "source": [
    "### The code below needs to be run on your own laptop to convert the names of the scare_crow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory and dataset splits\n",
    "base_dir = \"scarecrow_dataset\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Allowed image extensions (all lowercased for matching)\n",
    "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\n Processing {split} split...\")\n",
    "    \n",
    "    image_dir = os.path.join(base_dir, split, \"images\")\n",
    "    annotation_path = os.path.join(base_dir, split, \"annotations.json\")\n",
    "\n",
    "    # Get image files (case-insensitive extension check)\n",
    "    image_files = sorted([\n",
    "        f for f in os.listdir(image_dir)\n",
    "        if os.path.splitext(f)[1].lower() in image_extensions\n",
    "    ])\n",
    "\n",
    "    print(f\" Found {len(image_files)} image files in '{split}/images'\")\n",
    "\n",
    "    # Load annotations\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    print(f\" Found {len(annotations)} annotation entries in annotations.json\")\n",
    "\n",
    "    # Ensure lengths match\n",
    "    if len(image_files) != len(annotations):\n",
    "        print(f\"  WARNING: Number of images ≠ number of annotations in {split}\")\n",
    "        min_len = min(len(image_files), len(annotations))\n",
    "        print(f\" Truncating to {min_len} entries to avoid mismatch.\")\n",
    "        image_files = image_files[:min_len]\n",
    "        annotations = annotations[:min_len]\n",
    "    else:\n",
    "        min_len = len(image_files)\n",
    "\n",
    "    # Detect if already renamed\n",
    "    expected_prefix = f\"{split}_original_\"\n",
    "    already_renamed = all(f.startswith(expected_prefix) for f in image_files)\n",
    "\n",
    "    if already_renamed:\n",
    "        print(f\" Skipping {split} — already renamed.\")\n",
    "        continue\n",
    "\n",
    "    # Perform renaming\n",
    "    for idx, old_name in enumerate(image_files, 1):\n",
    "        ext = os.path.splitext(old_name)[1]\n",
    "        new_name = f\"{split}_original_{idx}{ext.lower()}\"\n",
    "        old_path = os.path.join(image_dir, old_name)\n",
    "        new_path = os.path.join(image_dir, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        image_files[idx - 1] = new_name  # Update name in list\n",
    "\n",
    "    # Update annotations\n",
    "    for idx, entry in enumerate(annotations):\n",
    "        entry[\"OriginalFileName\"] = image_files[idx]\n",
    "\n",
    "    # Save updated annotations\n",
    "    with open(annotation_path, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)\n",
    "\n",
    "    print(f\" {split} renamed and annotations updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9182d00",
   "metadata": {},
   "source": [
    "### Data loading & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8585276",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Defining a CustomDataset class ###\n",
    "######################################\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform = None):\n",
    "        \"\"\"\n",
    "        Initialize the custom dataset.\n",
    "        Works for both the train data and the test data.\n",
    "        \"\"\"\n",
    "        self.images_dir = os.path.join(data_path, \"images\")\n",
    "        self.transform = transform\n",
    "        annotations_file = data_path + \"/annotations.json\"\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            annotations_list = json.load(f)\n",
    "       \n",
    "        # We need to extract the bounding boxes of the annotations from the JSON file and store them as [x_min, y_min, x_max, y_max] tensors\n",
    "        self.data = []\n",
    "        for entry in annotations_list:\n",
    "            image_name = entry['OriginalFileName']\n",
    "            annotation_data = entry['AnnotationData']\n",
    "            bird_boxes = self.extract_bird_boxes(annotation_data)\n",
    "            self.data.append({'imagename': image_name, 'bird_boxes_tensor': bird_boxes})\n",
    "\n",
    "        # Note: we should not load all the images into a tensor here, as it would take too much memory. We load images into a tensor in the __getitem__ method.\n",
    "\n",
    "\n",
    "    def extract_bird_boxes(self, annotation_data):\n",
    "        \"\"\"\n",
    "        Extract the coordinates of the birds from the annotation data in the JSON file and return it as a tensor.\n",
    "        \"\"\"\n",
    "        bird_boxes = []\n",
    "        for entry in annotation_data:\n",
    "            if entry['Label'] == 'Bird':\n",
    "                coordinates_list = entry['Coordinates']\n",
    "                x_coordinates = [point['X'] for point in coordinates_list]\n",
    "                y_coordinates = [point['Y'] for point in coordinates_list]\n",
    "                x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "                y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "                bird_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return torch.tensor(bird_boxes, dtype=torch.float32) # Shape: (num_birds, 4)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset, i.e. the number of images.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load an image and its corresponding annotations.\n",
    "        Returns the image and a target dictionary with bounding boxes and labels (we need this for compatiblity with object detection models like Faster R-CNN)\n",
    "        \"\"\"\n",
    "        item = self.data[index]\n",
    "        image_path = os.path.join(self.images_dir, item['imagename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        bird_boxes = item['bird_boxes_tensor']\n",
    "        labels = torch.ones((bird_boxes.shape[0],), dtype=torch.int64) # Assuming all the labels are 'Bird' --> we assign this to class 1\n",
    "        target = {'boxes': bird_boxes, 'labels': labels} # should contain the bounding boxes and the labels\n",
    "\n",
    "        # Apply data augmentations\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### Finding the mean and std of the dataset ###\n",
    "###############################################\n",
    "\n",
    "def calculate_mean_and_std(dataset):\n",
    "    # Initialize sums for mean and variance\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for image, _ in tqdm(dataset, desc=\"Calculating Mean and Std\", unit=\"image\"):\n",
    "        # Convert image to tensor if it is in PIL format\n",
    "        image = transforms.ToTensor()(image)  # shape: (C, H, W)\n",
    "        \n",
    "        # Calculate the sum and squared sum of pixels for each channel\n",
    "        mean += image.mean([1, 2])  # mean per channel (C,)\n",
    "        std += image.std([1, 2])    # std per channel (C,)\n",
    "        num_pixels += 1\n",
    "    \n",
    "    # Average the sums to get the mean and std\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# For now, do no transformations:\n",
    "train_data_original = CustomDataset(\"scarecrow_dataset/train\", transform=None)\n",
    "train_data_extra = CustomDataset(\"bird-detection-farm/train\", transform=None)\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "\n",
    "mean, std = calculate_mean_and_std(train_data)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165af7f",
   "metadata": {},
   "source": [
    "Dataset Mean: tensor([0.5390, 0.5306, 0.4421])\n",
    "\n",
    "Dataset Std: tensor([0.1624, 0.1527, 0.1647])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c97e9b",
   "metadata": {},
   "source": [
    "### With the extra data\n",
    "\n",
    "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
    "\n",
    "Dataset Std: tensor([0.1674, 0.1557, 0.1689])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Defining a CustomTransformation class ###\n",
    "##############################################\n",
    "\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "mean = [0.5409, 0.5505, 0.3894]\n",
    "std = [0.1674, 0.1557, 0.1689]\n",
    "\n",
    "class CustomTransformation:\n",
    "    def __init__(self):\n",
    "        self.transforms = transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean, std)])\n",
    "        \n",
    "\n",
    "    def perform_horizontal_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Horizontally flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.hflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            width, _ = image.size\n",
    "            x_min = boxes[:, 0].clone()\n",
    "            x_max = boxes[:, 2].clone()\n",
    "            boxes[:, 0] = width - x_max\n",
    "            boxes[:, 2] = width - x_min\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_vertical_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Vertically flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.vflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            _, height = image.size\n",
    "            y_min = boxes[:, 1].clone()\n",
    "            y_max = boxes[:, 3].clone()\n",
    "            boxes[:, 1] = height - y_max\n",
    "            boxes[:, 3] = height - y_min\n",
    "            target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_random_rotation(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Perform a random rotation in multiples of 90 degrees with a given probability, default is 0.25.\n",
    "        \"\"\"\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice([90, 180, 270])\n",
    "\n",
    "            image = functional.rotate(image, angle)\n",
    "\n",
    "            boxes = target['boxes']\n",
    "            width, height = image.size\n",
    "\n",
    "            if angle == 90:\n",
    "                boxes = boxes[:, [1, 0, 3, 2]] # swap x and y coordinates\n",
    "                boxes[:, [0, 2]] = height - boxes[:, [2, 0]] # adjut x-coordinates because the rotation changes the origin\n",
    "            elif angle == 180:\n",
    "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]] # adjust x-coordinates\n",
    "                boxes[:, [1, 3]] = height - boxes[:, [3, 1]] # adjust y-coordinates\n",
    "            elif angle == 270:\n",
    "                boxes = boxes[:, [1, 0, 3, 2]] # swap x and y coordinates\n",
    "                boxes[:, [1, 3]] = width - boxes[:, [3, 1]] # adjust y-coordinates because the rotation changes to origin\n",
    "            target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def perform_random_resize(self, image, target, scale_range=(0.75, 1.25)):\n",
    "        \"\"\"\n",
    "        Perform a random reize within the specified scale range, default scale range is (0,75, 1.25)\n",
    "        \"\"\"\n",
    "        scale = random.uniform(*scale_range)\n",
    "\n",
    "        # Resize the image\n",
    "        width, height = image.size\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        image = functional.resize(image, [new_height, new_width])\n",
    "        \n",
    "        # Resize the boxes\n",
    "        boxes = target['boxes']\n",
    "        boxes = boxes * scale\n",
    "        target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        Apply the transformations to an image\n",
    "        \"\"\"\n",
    "        image, target = self.perform_horizontal_flip(image, target)\n",
    "        image, target = self.perform_vertical_flip(image, target)\n",
    "        image, target = self.perform_random_rotation(image, target)\n",
    "        image, target = self.perform_random_resize(image, target)\n",
    "        image = self.transforms(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Define paths\n",
    "train_data_original_path = \"scarecrow_dataset/train\"\n",
    "test_data_original_path = \"scarecrow_dataset/test\"\n",
    "val_data_original_path = \"scarecrow_dataset/val\"\n",
    "train_data_extra_path = \"bird-detection-farm/train\"\n",
    "valid_data_extra_path = \"bird-detection-farm/valid\"\n",
    "test_data_extra_path = \"bird-detection-farm/test\"\n",
    "\n",
    "# Loading the datasets with the transformations\n",
    "transform = CustomTransformation()\n",
    "\n",
    "train_data_original = CustomDataset(train_data_original_path, transform)\n",
    "valid_data_original = CustomDataset(val_data_original_path, transform)\n",
    "test_data_original = CustomDataset(test_data_original_path, transform)\n",
    "\n",
    "train_data_extra = CustomDataset(train_data_extra_path, transform)\n",
    "valid_data_extra = CustomDataset(valid_data_extra_path, transform)\n",
    "test_data_extra = CustomDataset(test_data_extra_path, transform)\n",
    "\n",
    "# Split old training set into train/val\n",
    "#train_data_original, valid_data_original = torch.utils.data.random_split(train_data_original, [0.8, 0.2])\n",
    "\n",
    "# Combine datasets\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "valid_data = torch.utils.data.ConcatDataset([valid_data_original, valid_data_extra])\n",
    "test_data = torch.utils.data.ConcatDataset([test_data_original, test_data_extra])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# No shuffling for validation and test data because we want consistnt order for reproducibility:\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
