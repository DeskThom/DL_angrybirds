{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f9751c",
   "metadata": {},
   "source": [
    "# Deep Learning group assignment\n",
    "Group name: Angry Birds\n",
    "\n",
    "Group members:\n",
    "- Nienke Reijnen: 2117034\n",
    "- Andrea Ciavatti: 2115635\n",
    "- Niels Boonstra: 1451294\n",
    "- Yannick Lankhorst: 2052754\n",
    "- Thom Zoomer:2059225\n",
    "- Anne Barnasconi: 2053988"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc615d47",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1df5d",
   "metadata": {},
   "source": [
    "Before running, make sure to also have installed the following packages (according to lab 8 instructions):\n",
    "- pip install imageio\n",
    "- pip install future\n",
    "- pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b751e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.transforms import functional\n",
    "from torch.utils.data import ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import itertools\n",
    "# from ultralytics import YOLO\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, device_name = (torch.device(\"cuda\"), torch.cuda.get_device_name(0)) if torch.cuda.is_available() else (torch.device(\"cpu\"), \"CPU\")\n",
    "print(f\"Device: {device}, {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f9ca2",
   "metadata": {},
   "source": [
    "## Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0c073",
   "metadata": {},
   "source": [
    "- DONE Correct implementation of data loaders for images and annotations for your specific object detection model\n",
    "- DONE Use of data augmentation techniques\n",
    "- DONE Appropriate shuffling  and batching of data\n",
    "- TO DO: Conduct an online search for relevant open-source datasets, and if you can find them, use them in your application as additional training data (to improve generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999552a",
   "metadata": {},
   "source": [
    "### The code below needs to be run on your own laptop to convert the names of the scare_crow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aff98e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train split...\n",
      "Found 263 annotation entries in annotations.json\n",
      "Renamed Schermafbeelding 2024-06-04 105530.png to train_original_1.png\n",
      "Renamed Schermafbeelding 2024-06-04 102707.png to train_original_2.png\n",
      "Renamed Schermafbeelding 2024-06-04 102122.png to train_original_3.png\n",
      "Renamed Schermafbeelding 2024-06-04 105217.png to train_original_4.png\n",
      "Renamed Schermafbeelding 2024-06-04 104532.png to train_original_5.png\n",
      "Renamed Schermafbeelding 2024-06-04 102227.png to train_original_6.png\n",
      "Renamed Schermafbeelding 2024-06-04 102744.png to train_original_7.png\n",
      "Renamed Schermafbeelding 2024-06-04 104541.png to train_original_8.png\n",
      "Renamed Schermafbeelding 2024-06-04 101800.png to train_original_9.png\n",
      "Renamed Schermafbeelding 2024-06-04 102731.png to train_original_10.png\n",
      "Renamed Schermafbeelding 2024-06-04 105154.png to train_original_11.png\n",
      "Renamed Schermafbeelding 2024-06-04 105244.png to train_original_12.png\n",
      "Renamed Schermafbeelding 2024-06-04 101719.png to train_original_13.png\n",
      "Renamed Schermafbeelding 2024-06-04 101700.png to train_original_14.png\n",
      "Renamed Schermafbeelding 2024-06-04 102006.png to train_original_15.png\n",
      "Renamed Schermafbeelding 2024-06-04 102504.png to train_original_16.png\n",
      "Renamed Schermafbeelding 2024-06-04 102404.png to train_original_17.png\n",
      "Renamed Schermafbeelding 2024-06-04 102439.png to train_original_18.png\n",
      "Renamed Schermafbeelding 2024-06-04 101952.png to train_original_19.png\n",
      "Renamed Schermafbeelding 2024-06-04 104613.png to train_original_20.png\n",
      "Renamed Schermafbeelding 2024-06-04 102556.png to train_original_21.png\n",
      "Renamed Schermafbeelding 2024-06-04 104738.png to train_original_22.png\n",
      "Renamed Schermafbeelding 2024-06-04 105259.png to train_original_23.png\n",
      "Renamed Schermafbeelding 2024-06-04 105010.png to train_original_24.png\n",
      "Renamed Schermafbeelding 2024-06-04 104932.png to train_original_25.png\n",
      "Renamed Schermafbeelding 2024-06-04 104504.png to train_original_26.png\n",
      "Renamed Schermafbeelding 2024-06-04 104559.png to train_original_27.png\n",
      "Renamed Schermafbeelding 2024-06-04 102348.png to train_original_28.png\n",
      "Renamed Schermafbeelding 2024-06-04 102419.png to train_original_29.png\n",
      "Renamed Schermafbeelding 2024-06-04 101908.png to train_original_30.png\n",
      "Renamed Schermafbeelding 2024-06-04 102300.png to train_original_31.png\n",
      "Renamed Schermafbeelding 2024-06-04 102634.png to train_original_32.png\n",
      "Renamed Schermafbeelding 2024-06-04 104453.png to train_original_33.png\n",
      "Renamed Schermafbeelding 2024-06-04 102135.png to train_original_34.png\n",
      "Renamed Schermafbeelding 2024-06-04 101731.png to train_original_35.png\n",
      "Renamed Schermafbeelding 2024-06-04 105202.png to train_original_36.png\n",
      "Renamed Schermafbeelding 2024-06-04 105121.png to train_original_37.png\n",
      "Renamed Schermafbeelding 2024-06-04 102100.png to train_original_38.png\n",
      "Renamed Schermafbeelding 2024-06-04 102315.png to train_original_39.png\n",
      "Renamed Schermafbeelding 2024-06-04 105327.png to train_original_40.png\n",
      "Renamed Schermafbeelding 2024-06-04 102246.png to train_original_41.png\n",
      "Renamed Schermafbeelding 2024-06-04 101823.png to train_original_42.png\n",
      "Renamed Schermafbeelding 2024-06-04 104713.png to train_original_43.png\n",
      "Renamed Schermafbeelding 2024-06-04 104704.png to train_original_44.png\n",
      "Renamed Schermafbeelding 2024-06-04 102034.png to train_original_45.png\n",
      "Renamed Schermafbeelding 2024-06-04 104521.png to train_original_46.png\n",
      "Renamed Schermafbeelding 2024-06-04 102620.png to train_original_47.png\n",
      "Renamed Schermafbeelding 2024-06-04 105309.png to train_original_48.png\n",
      "Renamed Schermafbeelding 2024-06-04 102210.png to train_original_49.png\n",
      "Renamed Schermafbeelding 2024-06-04 104909.png to train_original_50.png\n",
      "Renamed Schermafbeelding 2024-06-04 104923.png to train_original_51.png\n",
      "Renamed Schermafbeelding 2024-06-04 102529.png to train_original_52.png\n",
      "Renamed 20240903084813_0269_D_frame_2880 - kopie.png to train_original_53.png\n",
      "Renamed 20240901120856_0268_D_frame_540 - kopie.png to train_original_54.png\n",
      "Renamed 20240903084813_0269_D_frame_8760 - kopie.png to train_original_55.png\n",
      "Renamed 20240903090245_0271_D_frame_1410 - kopie.png to train_original_56.png\n",
      "Renamed 20240901115456_0265_D_frame_4590 - kopie.png to train_original_57.png\n",
      "Renamed 20240901120856_0268_D_frame_1470 - kopie.png to train_original_58.png\n",
      "Renamed 20240903085339_0270_D_frame_420 - kopie.png to train_original_59.png\n",
      "Renamed 20240901120856_0268_D_frame_1170 - kopie.png to train_original_60.png\n",
      "Renamed 20240903085339_0270_D_frame_3840 - kopie.png to train_original_61.png\n",
      "Renamed 20240903085339_0270_D_frame_2310 - kopie.png to train_original_62.png\n",
      "Renamed 20240903085339_0270_D_frame_4020 - kopie.png to train_original_63.png\n",
      "Renamed 20240903084813_0269_D_frame_420 - kopie.png to train_original_64.png\n",
      "Renamed 20240903084813_0269_D_frame_9600 - kopie.png to train_original_65.png\n",
      "Renamed 20240903084813_0269_D_frame_2820 - kopie.png to train_original_66.png\n",
      "Renamed 20240903085339_0270_D_frame_780 - kopie.png to train_original_67.png\n",
      "Renamed 20240903084813_0269_D_frame_2400 - kopie.png to train_original_68.png\n",
      "Renamed 20240903084813_0269_D_frame_4020 - kopie.png to train_original_69.png\n",
      "Renamed 20240903085339_0270_D_frame_2280 - kopie.png to train_original_70.png\n",
      "Renamed 20240903084813_0269_D_frame_2520 - kopie.png to train_original_71.png\n",
      "Renamed 20240903085339_0270_D_frame_4950 - kopie.png to train_original_72.png\n",
      "Renamed 20240901120856_0268_D_frame_1500 - kopie.png to train_original_73.png\n",
      "Renamed 20240903085339_0270_D_frame_3570 - kopie.png to train_original_74.png\n",
      "Renamed 20240903085339_0270_D_frame_3600 - kopie.png to train_original_75.png\n",
      "Renamed 20240903085339_0270_D_frame_3210 - kopie.png to train_original_76.png\n",
      "Renamed 20240901115456_0265_D_frame_4560 - kopie.png to train_original_77.png\n",
      "Renamed 20240903090245_0271_D_frame_1440 - kopie.png to train_original_78.png\n",
      "Renamed 20240903084813_0269_D_frame_8100 - kopie.png to train_original_79.png\n",
      "Renamed 20240903085339_0270_D_frame_4290 - kopie.png to train_original_80.png\n",
      "Renamed 20240903084813_0269_D_frame_2280 - kopie.png to train_original_81.png\n",
      "Renamed 20240903084813_0269_D_frame_2040 - kopie.png to train_original_82.png\n",
      "Renamed 20240903085339_0270_D_frame_5520 - kopie.png to train_original_83.png\n",
      "Renamed 20240901120230_0267_D_frame_630 - kopie.png to train_original_84.png\n",
      "Renamed 20240903085339_0270_D_frame_60 - kopie.png to train_original_85.png\n",
      "Renamed 20240903085339_0270_D_frame_750 - kopie.png to train_original_86.png\n",
      "Renamed 20240903085339_0270_D_frame_4410 - kopie.png to train_original_87.png\n",
      "Renamed 20240903084813_0269_D_frame_5160 - kopie.png to train_original_88.png\n",
      "Renamed 20240901115456_0265_D_frame_1170 - kopie.png to train_original_89.png\n",
      "Renamed 20240903084813_0269_D_frame_2760 - kopie.png to train_original_90.png\n",
      "Renamed 20240903085339_0270_D_frame_4350 - kopie.png to train_original_91.png\n",
      "Renamed 20240903090245_0271_D_frame_930 - kopie.png to train_original_92.png\n",
      "Renamed 20240901120856_0268_D_frame_930 - kopie.png to train_original_93.png\n",
      "Renamed 20240903090245_0271_D_frame_900 - kopie.png to train_original_94.png\n",
      "Renamed 20240903085339_0270_D_frame_4470 - kopie.png to train_original_95.png\n",
      "Renamed 20240901120856_0268_D_frame_1380 - kopie.png to train_original_96.png\n",
      "Renamed 20240903085339_0270_D_frame_2130 - kopie.png to train_original_97.png\n",
      "Renamed 20240903085339_0270_D_frame_5460 - kopie.png to train_original_98.png\n",
      "Renamed 20240903084813_0269_D_frame_2940 - kopie.png to train_original_99.png\n",
      "Renamed 20240903085339_0270_D_frame_3120 - kopie.png to train_original_100.png\n",
      "Renamed 20240903085339_0270_D_frame_4380 - kopie.png to train_original_101.png\n",
      "Renamed 20240903084813_0269_D_frame_2460 - kopie.png to train_original_102.png\n",
      "Renamed 20240903085339_0270_D_frame_1200 - kopie.png to train_original_103.png\n",
      "Renamed 20240903084813_0269_D_frame_9480 - kopie.png to train_original_104.png\n",
      "Renamed 20240903085339_0270_D_frame_4110 - kopie.png to train_original_105.png\n",
      "Renamed 20240901115456_0265_D_frame_1230 - kopie.png to train_original_106.png\n",
      "Renamed 20240903084813_0269_D_frame_9720 - kopie.png to train_original_107.png\n",
      "Renamed 20240903085339_0270_D_frame_3090 - kopie.png to train_original_108.png\n",
      "Renamed 20240903085339_0270_D_frame_1920 - kopie.png to train_original_109.png\n",
      "Renamed 20240903084813_0269_D_frame_8880 - kopie.png to train_original_110.png\n",
      "Renamed 20240903085339_0270_D_frame_3540 - kopie.png to train_original_111.png\n",
      "Renamed 20240903084813_0269_D_frame_1620 - kopie.png to train_original_112.png\n",
      "Renamed 20240906133805_0281_D_frame_2160.png to train_original_113.png\n",
      "Renamed 20240904083832_0272_D_frame_840.png to train_original_114.png\n",
      "Renamed 20240903171845_0037_D_frame_3180.png to train_original_115.png\n",
      "Renamed 20240906133805_0281_D_frame_1980.png to train_original_116.png\n",
      "Renamed 20240906133805_0281_D_frame_1800.png to train_original_117.png\n",
      "Renamed 20240903171845_0037_D_frame_720.png to train_original_118.png\n",
      "Renamed 20240904110435_0274_D_frame_1140.png to train_original_119.png\n",
      "Renamed 20240904083832_0272_D_frame_6720.png to train_original_120.png\n",
      "Renamed 20240904110435_0274_D_frame_1020.png to train_original_121.png\n",
      "Renamed 20240904083832_0272_D_frame_6600.png to train_original_122.png\n",
      "Renamed 20240906133238_0280_D_frame_6960.png to train_original_123.png\n",
      "Renamed 20240906133805_0281_D_frame_3180.png to train_original_124.png\n",
      "Renamed 20240906133805_0281_D_frame_540.png to train_original_125.png\n",
      "Renamed 20240904083832_0272_D_frame_2880.png to train_original_126.png\n",
      "Renamed 20240903171845_0037_D_frame_1200.png to train_original_127.png\n",
      "Renamed 20240904110435_0274_D_frame_780.png to train_original_128.png\n",
      "Renamed 20240906133238_0280_D_frame_5640.png to train_original_129.png\n",
      "Renamed 20240904083832_0272_D_frame_3000.png to train_original_130.png\n",
      "Renamed 20240903171845_0037_D_frame_1080.png to train_original_131.png\n",
      "Renamed 20240906133805_0281_D_frame_1260.png to train_original_132.png\n",
      "Renamed 20240906133805_0281_D_frame_1500.png to train_original_133.png\n",
      "Renamed 20240904110435_0274_D_frame_2940.png to train_original_134.png\n",
      "Renamed 20240906133805_0281_D_frame_3900.png to train_original_135.png\n",
      "Renamed 20240904083832_0272_D_frame_4200.png to train_original_136.png\n",
      "Renamed 20240903171845_0037_D_frame_780.png to train_original_137.png\n",
      "Renamed 20240903171845_0037_D_frame_2940.png to train_original_138.png\n",
      "Renamed 20240903171845_0037_D_frame_900.png to train_original_139.png\n",
      "Renamed 20240906133238_0280_D_frame_3480.png to train_original_140.png\n",
      "Renamed 20240904110435_0274_D_frame_720.png to train_original_141.png\n",
      "Renamed 20240904083832_0272_D_frame_4560.png to train_original_142.png\n",
      "Renamed 20240906133238_0280_D_frame_4920.png to train_original_143.png\n",
      "Renamed 20240904083832_0272_D_frame_6960.png to train_original_144.png\n",
      "Renamed 20240904110435_0274_D_frame_120.png to train_original_145.png\n",
      "Renamed 20240906133805_0281_D_frame_60.png to train_original_146.png\n",
      "Renamed 20240906133238_0280_D_frame_7200.png to train_original_147.png\n",
      "Renamed 20240904110435_0274_D_frame_2160.png to train_original_148.png\n",
      "Renamed 20240906133805_0281_D_frame_1320.png to train_original_149.png\n",
      "Renamed 20240904110435_0274_D_frame_960.png to train_original_150.png\n",
      "Renamed 20240904110435_0274_D_frame_1260.png to train_original_151.png\n",
      "Renamed 20240906133238_0280_D_frame_4080.png to train_original_152.png\n",
      "Renamed 20240903171845_0037_D_frame_1020.png to train_original_153.png\n",
      "Renamed 20240903171845_0037_D_frame_2400.png to train_original_154.png\n",
      "Renamed 20240903171845_0037_D_frame_3420.png to train_original_155.png\n",
      "Renamed 20240903171845_0037_D_frame_960.png to train_original_156.png\n",
      "Renamed 20240906133805_0281_D_frame_120.png to train_original_157.png\n",
      "Renamed 20240906133805_0281_D_frame_840.png to train_original_158.png\n",
      "Renamed 20240904084357_0273_D_frame_240.png to train_original_159.png\n",
      "Renamed 20240903171845_0037_D_frame_840.png to train_original_160.png\n",
      "Renamed 20240904083832_0272_D_frame_2760.png to train_original_161.png\n",
      "Renamed 20240906133238_0280_D_frame_3600.png to train_original_162.png\n",
      "Renamed 20240906133238_0280_D_frame_6120.png to train_original_163.png\n",
      "Renamed 20240906133805_0281_D_frame_3780.png to train_original_164.png\n",
      "Renamed 20240904083832_0272_D_frame_1320.png to train_original_165.png\n",
      "Renamed 20240906133238_0280_D_frame_4440.png to train_original_166.png\n",
      "Renamed 20240906133238_0280_D_frame_3720.png to train_original_167.png\n",
      "Renamed 20240904083832_0272_D_frame_5880.png to train_original_168.png\n",
      "Renamed 20240904110435_0274_D_frame_0.png to train_original_169.png\n",
      "Renamed 20240903171845_0037_D_frame_3060.png to train_original_170.png\n",
      "Renamed 20240904083832_0272_D_frame_6240.png to train_original_171.png\n",
      "Renamed 20240904110435_0274_D_frame_2220.png to train_original_172.png\n",
      "Renamed 20240904083832_0272_D_frame_5640.png to train_original_173.png\n",
      "Renamed 20240906133805_0281_D_frame_3960.png to train_original_174.png\n",
      "Renamed 20240906133805_0281_D_frame_4140.png to train_original_175.png\n",
      "Renamed 20240904083832_0272_D_frame_1080.png to train_original_176.png\n",
      "Renamed 20240903171845_0037_D_frame_3300.png to train_original_177.png\n",
      "Renamed 20240904083832_0272_D_frame_3360.png to train_original_178.png\n",
      "Renamed 20240906133238_0280_D_frame_6480.png to train_original_179.png\n",
      "Renamed 20240906133238_0280_D_frame_7800.png to train_original_180.png\n",
      "Renamed 20240906133238_0280_D_frame_840.png to train_original_181.png\n",
      "Renamed DJI_0322.JPG to train_original_182.jpg\n",
      "Renamed DJI_0316.JPG to train_original_183.jpg\n",
      "Renamed DJI_0320.JPG to train_original_184.jpg\n",
      "Renamed DJI_0318.JPG to train_original_185.jpg\n",
      "Renamed DJI_0216_frame_300.png to train_original_186.png\n",
      "Renamed DJI_0186.JPG to train_original_187.jpg\n",
      "Renamed DJI_0315.JPG to train_original_188.jpg\n",
      "Renamed DJI_0216_frame_180.png to train_original_189.png\n",
      "Renamed DJI_0312.JPG to train_original_190.jpg\n",
      "Renamed DJI_0317.JPG to train_original_191.jpg\n",
      "Renamed DJI_0311.JPG to train_original_192.jpg\n",
      "Renamed DJI_0329.JPG to train_original_193.jpg\n",
      "Renamed DJI_0101.JPG to train_original_194.jpg\n",
      "Renamed DJI_0330.JPG to train_original_195.jpg\n",
      "Renamed DJI_0336.JPG to train_original_196.jpg\n",
      "Renamed DJI_0324.JPG to train_original_197.jpg\n",
      "Renamed DJI_0216_frame_240.png to train_original_198.png\n",
      "Renamed DJI_0310.JPG to train_original_199.jpg\n",
      "Renamed DJI_0345.JPG to train_original_200.jpg\n",
      "Renamed DJI_0313.JPG to train_original_201.jpg\n",
      "Renamed DJI_0335.JPG to train_original_202.jpg\n",
      "Renamed DJI_0333.JPG to train_original_203.jpg\n",
      "Renamed DJI_0216_frame_120.png to train_original_204.png\n",
      "Renamed DJI_0334.JPG to train_original_205.jpg\n",
      "Renamed DJI_0164.JPG to train_original_206.jpg\n",
      "Renamed DJI_0321.JPG to train_original_207.jpg\n",
      "Renamed vlcsnap-2024-11-15-11h59m36s811.png to train_original_208.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m31s299.png to train_original_209.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m18s782.png to train_original_210.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m32s623.png to train_original_211.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m13s782.png to train_original_212.png\n",
      "Renamed vlcsnap-2024-11-15-11h49m58s948.png to train_original_213.png\n",
      "Renamed vlcsnap-2024-11-15-11h52m53s317.png to train_original_214.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m26s591.png to train_original_215.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m11s737.png to train_original_216.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m32s948.png to train_original_217.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m35s427.png to train_original_218.png\n",
      "Renamed vlcsnap-2024-11-15-11h58m32s761.png to train_original_219.png\n",
      "Renamed vlcsnap-2024-11-15-11h58m01s594.png to train_original_220.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m01s381.png to train_original_221.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m32s418.png to train_original_222.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m40s634.png to train_original_223.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m04s332.png to train_original_224.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m19s743.png to train_original_225.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m47s659.png to train_original_226.png\n",
      "Renamed vlcsnap-2024-11-15-11h58m41s474.png to train_original_227.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m25s573.png to train_original_228.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m06s399.png to train_original_229.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m08s727.png to train_original_230.png\n",
      "Renamed vlcsnap-2024-11-15-11h52m31s064.png to train_original_231.png\n",
      "Renamed vlcsnap-2024-11-15-11h52m35s182.png to train_original_232.png\n",
      "Renamed vlcsnap-2024-11-15-11h58m05s793.png to train_original_233.png\n",
      "Renamed vlcsnap-2024-11-15-11h48m59s570.png to train_original_234.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m03s318.png to train_original_235.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m16s733.png to train_original_236.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m20s772.png to train_original_237.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m22s031.png to train_original_238.png\n",
      "Renamed vlcsnap-2024-11-15-11h55m47s464.png to train_original_239.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m22s161.png to train_original_240.png\n",
      "Renamed vlcsnap-2024-11-15-11h52m58s755.png to train_original_241.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m14s934.png to train_original_242.png\n",
      "Renamed vlcsnap-2024-11-15-11h55m42s638.png to train_original_243.png\n",
      "Renamed vlcsnap-2024-11-15-11h52m45s763.png to train_original_244.png\n",
      "Renamed vlcsnap-2024-11-15-11h55m45s875.png to train_original_245.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m05s685.png to train_original_246.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m46s339.png to train_original_247.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m34s193.png to train_original_248.png\n",
      "Renamed vlcsnap-2024-11-15-11h55m12s636.png to train_original_249.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m30s608.png to train_original_250.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m05s261.png to train_original_251.png\n",
      "Renamed vlcsnap-2024-11-15-11h54m18s074.png to train_original_252.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m22s485.png to train_original_253.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m55s943.png to train_original_254.png\n",
      "Renamed DJI_0358_frame_450.png to train_original_255.png\n",
      "Renamed DJI_0358_frame_1050.png to train_original_256.png\n",
      "Renamed DJI_0358_frame_480.png to train_original_257.png\n",
      "Renamed DJI_0358_frame_1830.png to train_original_258.png\n",
      "Renamed DJI_0358_frame_330.png to train_original_259.png\n",
      "Renamed DJI_0358_frame_1800.png to train_original_260.png\n",
      "Renamed DJI_0358_frame_390.png to train_original_261.png\n",
      "Renamed DJI_0358_frame_420.png to train_original_262.png\n",
      "Renamed DJI_0358_frame_300.png to train_original_263.png\n",
      "train renamed and annotations updated.\n",
      "\n",
      "Processing val split...\n",
      "Found 32 annotation entries in annotations.json\n",
      "Renamed Schermafbeelding 2024-06-04 104831.png to val_original_1.png\n",
      "Renamed Schermafbeelding 2024-06-04 104857.png to val_original_2.png\n",
      "Renamed Schermafbeelding 2024-06-04 104648.png to val_original_3.png\n",
      "Renamed Schermafbeelding 2024-06-04 105612.png to val_original_4.png\n",
      "Renamed Schermafbeelding 2024-06-04 105136.png to val_original_5.png\n",
      "Renamed Schermafbeelding 2024-06-04 105402.png to val_original_6.png\n",
      "Renamed Schermafbeelding 2024-06-04 105341.png to val_original_7.png\n",
      "Renamed Schermafbeelding 2024-06-04 104959.png to val_original_8.png\n",
      "Renamed 20240903085339_0270_D_frame_1890 - kopie.png to val_original_9.png\n",
      "Renamed 20240903090245_0271_D_frame_960 - kopie.png to val_original_10.png\n",
      "Renamed 20240903085339_0270_D_frame_3780 - kopie.png to val_original_11.png\n",
      "Renamed 20240903084813_0269_D_frame_3960 - kopie.png to val_original_12.png\n",
      "Renamed 20240901120856_0268_D_frame_1110 - kopie.png to val_original_13.png\n",
      "Renamed 20240901115456_0265_D_frame_1200 - kopie.png to val_original_14.png\n",
      "Renamed 20240903084813_0269_D_frame_4440 - kopie.png to val_original_15.png\n",
      "Renamed 20240904110435_0274_D_frame_2100.png to val_original_16.png\n",
      "Renamed 20240906133805_0281_D_frame_900.png to val_original_17.png\n",
      "Renamed 20240906133238_0280_D_frame_4320.png to val_original_18.png\n",
      "Renamed 20240904083832_0272_D_frame_6000.png to val_original_19.png\n",
      "Renamed 20240904083832_0272_D_frame_3840.png to val_original_20.png\n",
      "Renamed 20240906133238_0280_D_frame_3960.png to val_original_21.png\n",
      "Renamed 20240906133805_0281_D_frame_240.png to val_original_22.png\n",
      "Renamed 20240904083832_0272_D_frame_4320.png to val_original_23.png\n",
      "Renamed DJI_0331.JPG to val_original_24.jpg\n",
      "Renamed DJI_0332.JPG to val_original_25.jpg\n",
      "Renamed DJI_0319.JPG to val_original_26.jpg\n",
      "Renamed vlcsnap-2024-11-15-11h59m37s625.png to val_original_27.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m20s957.png to val_original_28.png\n",
      "Renamed vlcsnap-2024-11-15-11h49m00s549.png to val_original_29.png\n",
      "Renamed vlcsnap-2024-11-15-11h59m05s670.png to val_original_30.png\n",
      "Renamed vlcsnap-2024-11-15-11h50m08s654.png to val_original_31.png\n",
      "Renamed DJI_0358_frame_360.png to val_original_32.png\n",
      "val renamed and annotations updated.\n",
      "\n",
      "Processing test split...\n",
      "Found 31 annotation entries in annotations.json\n",
      "Renamed Schermafbeelding 2024-06-04 105113.png to test_original_1.png\n",
      "Renamed Schermafbeelding 2024-06-04 104630.png to test_original_2.png\n",
      "Renamed Schermafbeelding 2024-06-04 104842.png to test_original_3.png\n",
      "Renamed Schermafbeelding 2024-06-04 105353.png to test_original_4.png\n",
      "Renamed Schermafbeelding 2024-06-04 104944.png to test_original_5.png\n",
      "Renamed Schermafbeelding 2024-06-04 105432.png to test_original_6.png\n",
      "Renamed Schermafbeelding 2024-06-04 104818.png to test_original_7.png\n",
      "Renamed 20240901120856_0268_D_frame_1020 - kopie.png to test_original_8.png\n",
      "Renamed 20240903084813_0269_D_frame_3000 - kopie.png to test_original_9.png\n",
      "Renamed 20240903085339_0270_D_frame_3660 - kopie.png to test_original_10.png\n",
      "Renamed 20240901120856_0268_D_frame_1080 - kopie.png to test_original_11.png\n",
      "Renamed 20240903084813_0269_D_frame_1440 - kopie.png to test_original_12.png\n",
      "Renamed 20240903085339_0270_D_frame_4050 - kopie.png to test_original_13.png\n",
      "Renamed 20240903084813_0269_D_frame_1740 - kopie.png to test_original_14.png\n",
      "Renamed 20240906133805_0281_D_frame_3720.png to test_original_15.png\n",
      "Renamed 20240903171845_0037_D_frame_660.png to test_original_16.png\n",
      "Renamed 20240904083832_0272_D_frame_3720.png to test_original_17.png\n",
      "Renamed 20240903171845_0037_D_frame_3660.png to test_original_18.png\n",
      "Renamed 20240903171845_0037_D_frame_2760.png to test_original_19.png\n",
      "Renamed 20240904110435_0274_D_frame_1800.png to test_original_20.png\n",
      "Renamed 20240906133238_0280_D_frame_4200.png to test_original_21.png\n",
      "Renamed 20240903171845_0037_D_frame_3120.png to test_original_22.png\n",
      "Renamed DJI_0184.JPG to test_original_23.jpg\n",
      "Renamed DJI_0314.JPG to test_original_24.jpg\n",
      "Renamed DJI_0323.JPG to test_original_25.jpg\n",
      "Renamed vlcsnap-2024-11-15-11h59m39s766.png to test_original_26.png\n",
      "Renamed vlcsnap-2024-11-15-11h57m55s121.png to test_original_27.png\n",
      "Renamed vlcsnap-2024-11-15-11h52m39s652.png to test_original_28.png\n",
      "Renamed vlcsnap-2024-11-15-11h58m07s929.png to test_original_29.png\n",
      "Renamed vlcsnap-2024-11-15-11h56m23s608.png to test_original_30.png\n",
      "Renamed DJI_0358_frame_1020.png to test_original_31.png\n",
      "test renamed and annotations updated.\n"
     ]
    }
   ],
   "source": [
    "# Define base directory and dataset splits\n",
    "base_dir = \"datasets/scarecrow_dataset\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Allowed image extensions (all lowercased for matching)\n",
    "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\nProcessing {split} split...\")\n",
    "\n",
    "    image_dir = os.path.join(base_dir, split, \"images\")\n",
    "    annotation_path = os.path.join(base_dir, split, \"annotations.json\")\n",
    "\n",
    "    # Load annotations\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    print(f\"Found {len(annotations)} annotation entries in annotations.json\")\n",
    "\n",
    "    # Loop through annotations and rename corresponding images\n",
    "    for idx, annotation in enumerate(annotations, 1):\n",
    "        old_name = annotation[\"OriginalFileName\"]\n",
    "        ext = os.path.splitext(old_name)[1].lower()  # Preserve the file extension\n",
    "        new_name = f\"{split}_original_{idx}{ext}\"\n",
    "\n",
    "        old_path = os.path.join(image_dir, old_name)\n",
    "        new_path = os.path.join(image_dir, new_name)\n",
    "\n",
    "        # Check if the image file exists before renaming\n",
    "        if os.path.exists(old_path):\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed {old_name} to {new_name}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Image file {old_name} not found in {image_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Update the annotation with the new image name\n",
    "        annotation[\"OriginalFileName\"] = new_name\n",
    "\n",
    "    # Save updated annotations\n",
    "    with open(annotation_path, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)\n",
    "\n",
    "    print(f\"{split} renamed and annotations updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9182d00",
   "metadata": {},
   "source": [
    "### Data loading & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8585276",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Defining a CustomDataset class ###\n",
    "######################################\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform = None):\n",
    "        \"\"\"\n",
    "        Initialize the custom dataset.\n",
    "        Works for both the train data and the test data.\n",
    "        \"\"\"\n",
    "        self.images_dir = os.path.join(data_path, \"images\")\n",
    "        self.transform = transform\n",
    "        annotations_file = data_path + \"/annotations.json\"\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            annotations_list = json.load(f)\n",
    "       \n",
    "        # We need to extract the bounding boxes of the annotations from the JSON file and store them as [x_min, y_min, x_max, y_max] tensors\n",
    "        self.data = []\n",
    "        for entry in annotations_list:\n",
    "            image_name = entry['OriginalFileName']\n",
    "            annotation_data = entry['AnnotationData']\n",
    "            bird_boxes = self.extract_bird_boxes(annotation_data)\n",
    "            self.data.append({'imagename': image_name, 'bird_boxes_tensor': bird_boxes})\n",
    "\n",
    "        # Note: we should not load all the images into a tensor here, as it would take too much memory. We load images into a tensor in the __getitem__ method.\n",
    "\n",
    "\n",
    "    def extract_bird_boxes(self, annotation_data):\n",
    "        \"\"\"\n",
    "        Extract the coordinates of the birds from the annotation data in the JSON file and return it as a tensor.\n",
    "        \"\"\"\n",
    "        bird_boxes = []\n",
    "        for entry in annotation_data:\n",
    "            if entry['Label'] == 'Bird':\n",
    "                coordinates_list = entry['Coordinates']\n",
    "                x_coordinates = [point['X'] for point in coordinates_list]\n",
    "                y_coordinates = [point['Y'] for point in coordinates_list]\n",
    "                x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "                y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "                bird_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return torch.tensor(bird_boxes, dtype=torch.float32) # Shape: (num_birds, 4)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset, i.e. the number of images.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load an image and its corresponding annotations.\n",
    "        Returns the image and a target dictionary with bounding boxes and labels (we need this for compatiblity with object detection models like Faster R-CNN)\n",
    "        \"\"\"\n",
    "        item = self.data[index]\n",
    "        image_path = os.path.join(self.images_dir, item['imagename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        bird_boxes = item['bird_boxes_tensor']\n",
    "        labels = torch.ones((bird_boxes.shape[0],), dtype=torch.int64) # Assuming all the labels are 'Bird' --> we assign this to class 1\n",
    "        target = {'boxes': bird_boxes, 'labels': labels} # should contain the bounding boxes and the labels\n",
    "\n",
    "        # Apply data augmentations\n",
    "        if self.transform is not None:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480ec51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Mean and Std: 100%|██████████| 381/381 [01:55<00:00,  3.31image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
      "Dataset Std: tensor([0.1674, 0.1557, 0.1689])\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "### Finding the mean and std of the dataset ###\n",
    "###############################################\n",
    "\n",
    "def calculate_mean_and_std(dataset):\n",
    "    # Initialize sums for mean and variance\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for image, _ in tqdm(dataset, desc=\"Calculating Mean and Std\", unit=\"image\"):\n",
    "        # Convert image to tensor if it is in PIL format\n",
    "        image = transforms.ToTensor()(image)  # shape: (C, H, W)\n",
    "        \n",
    "        # Calculate the sum and squared sum of pixels for each channel\n",
    "        mean += image.mean([1, 2])  # mean per channel (C,)\n",
    "        std += image.std([1, 2])    # std per channel (C,)\n",
    "        num_pixels += 1\n",
    "    \n",
    "    # Average the sums to get the mean and std\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# For now, do no transformations:\n",
    "train_data_original = CustomDataset(\"datasets/scarecrow_dataset/train\", transform=None)\n",
    "train_data_extra = CustomDataset(\"datasets/bird-detection-farm/train\", transform=None)\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "\n",
    "mean, std = calculate_mean_and_std(train_data)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165af7f",
   "metadata": {},
   "source": [
    "Dataset Mean: tensor([0.5390, 0.5306, 0.4421])\n",
    "\n",
    "Dataset Std: tensor([0.1624, 0.1527, 0.1647])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c97e9b",
   "metadata": {},
   "source": [
    "### With the extra data\n",
    "\n",
    "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
    "\n",
    "Dataset Std: tensor([0.1674, 0.1557, 0.1689])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c519cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Defining a CustomTransformation class ###\n",
    "##############################################\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "mean = [0.5409, 0.5505, 0.3894]\n",
    "std = [0.1674, 0.1557, 0.1689]\n",
    "\n",
    "class CustomTransformation:\n",
    "    def __init__(self):\n",
    "        self.transforms = transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean, std)])\n",
    "        \n",
    "\n",
    "    def perform_horizontal_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Horizontally flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.hflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            width, _ = image.size\n",
    "            x_min = boxes[:, 0].clone()\n",
    "            x_max = boxes[:, 2].clone()\n",
    "            boxes[:, 0] = width - x_max\n",
    "            boxes[:, 2] = width - x_min\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_vertical_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Vertically flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.vflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            _, height = image.size\n",
    "            y_min = boxes[:, 1].clone()\n",
    "            y_max = boxes[:, 3].clone()\n",
    "            boxes[:, 1] = height - y_max\n",
    "            boxes[:, 3] = height - y_min\n",
    "            target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def rotate_box(self, boxes, angle, img_width, img_height):\n",
    "        # Rotate in the opposite (clockwise) direction to match torchvision's CCW rotation\n",
    "        angle_rad = math.radians(-angle)\n",
    "\n",
    "        cx, cy = img_width / 2, img_height / 2\n",
    "\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            x0, y0, x1, y1 = box.tolist()\n",
    "            corners = [\n",
    "                [x0, y0],\n",
    "                [x1, y0],\n",
    "                [x1, y1],\n",
    "                [x0, y1]\n",
    "            ]\n",
    "            rotated = []\n",
    "            for x, y in corners:\n",
    "                # Translate to origin\n",
    "                x -= cx\n",
    "                y -= cy\n",
    "                # Rotate\n",
    "                x_new = x * math.cos(angle_rad) - y * math.sin(angle_rad)\n",
    "                y_new = x * math.sin(angle_rad) + y * math.cos(angle_rad)\n",
    "                # Translate back\n",
    "                x_new += cx\n",
    "                y_new += cy\n",
    "                rotated.append([x_new, y_new])\n",
    "            rotated = torch.tensor(rotated)\n",
    "            x_min, y_min = rotated.min(dim=0).values\n",
    "            x_max, y_max = rotated.max(dim=0).values\n",
    "            new_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        return torch.tensor(new_boxes)\n",
    "\n",
    "\n",
    "    def perform_random_rotation(self, image, target, prob=0.25, rotations=[90, 180, 270]):\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice(rotations)\n",
    "            w, h = image.size\n",
    "            image = TF.rotate(image, angle)  # CCW rotation\n",
    "            boxes = target['boxes']\n",
    "            target['boxes'] = self.rotate_box(boxes, angle, w, h)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "    def perform_random_resize(self, image, target, scale_range=(0.75, 1.25)):\n",
    "        \"\"\"\n",
    "        Perform a random reize within the specified scale range, default scale range is (0,75, 1.25)\n",
    "        \"\"\"\n",
    "        scale = random.uniform(*scale_range)\n",
    "\n",
    "        # Resize the image\n",
    "        width, height = image.size\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        image = functional.resize(image, [new_height, new_width])\n",
    "        \n",
    "        # Resize the boxes\n",
    "        boxes = target['boxes']\n",
    "        boxes = boxes * scale\n",
    "        target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        Apply the transformations to an image\n",
    "        \"\"\"\n",
    "        image, target = self.perform_horizontal_flip(image, target)\n",
    "        image, target = self.perform_vertical_flip(image, target)\n",
    "        image, target = self.perform_random_rotation(image, target)\n",
    "        image, target = self.perform_random_resize(image, target)\n",
    "        image = self.transforms(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd78746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Define paths\n",
    "train_data_original_path = \"datasets/scarecrow_dataset/train\"\n",
    "test_data_original_path = \"datasets/scarecrow_dataset/test\"\n",
    "val_data_original_path = \"datasets/scarecrow_dataset/val\"\n",
    "train_data_extra_path = \"datasets/bird-detection-farm/train\"\n",
    "valid_data_extra_path = \"datasets/bird-detection-farm/valid\"\n",
    "test_data_extra_path = \"datasets/bird-detection-farm/test\"\n",
    "\n",
    "# Loading the datasets with the transformations\n",
    "transform = CustomTransformation()\n",
    "\n",
    "train_data_raw = CustomDataset(train_data_original_path)\n",
    "train_data_original = CustomDataset(train_data_original_path, transform)\n",
    "valid_data_original = CustomDataset(val_data_original_path, transform)\n",
    "test_data_original = CustomDataset(test_data_original_path, transform)\n",
    "\n",
    "train_data_extra = CustomDataset(train_data_extra_path, transform)\n",
    "valid_data_extra = CustomDataset(valid_data_extra_path, transform)\n",
    "test_data_extra = CustomDataset(test_data_extra_path, transform)\n",
    "\n",
    "# Split old training set into train/val\n",
    "#train_data_original, valid_data_original = torch.utils.data.random_split(train_data_original, [0.8, 0.2])\n",
    "\n",
    "# Combine datasets\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "valid_data = torch.utils.data.ConcatDataset([valid_data_original, valid_data_extra])\n",
    "test_data = torch.utils.data.ConcatDataset([test_data_original, test_data_extra])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# No shuffling for validation and test data because we want consistnt order for reproducibility:\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c387b815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch one image and its corresponding target from the dataset\n",
    "image, target = train_data_original[88]  # Replace 0 with any index to fetch a different image\n",
    "\n",
    "# Convert the image tensor to a NumPy array for visualization\n",
    "image_np = image.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "image_np = (image_np * std + mean).clip(0, 1)  # Denormalize the image\n",
    "\n",
    "# Visualize the image with bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image_np)\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in target['boxes']:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    rect = patches.Rectangle(\n",
    "        (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "        linewidth=2, edgecolor='r', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ea951",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### Debugging data augmentation ###\n",
    "###################################\n",
    "\n",
    "# import copy\n",
    "# import numpy as np\n",
    "\n",
    "# random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Choose the index of an image to test\n",
    "# idx = 55\n",
    "# image_raw, target_raw = train_data_raw[idx]\n",
    "\n",
    "# def visualize(image, boxes, title=\"Image\"):\n",
    "#     # Convert PIL image to NumPy\n",
    "#     if isinstance(image, torch.Tensor):\n",
    "#         image = image.permute(1, 2, 0).numpy()\n",
    "#     elif hasattr(image, 'size') and not isinstance(image, np.ndarray):\n",
    "#         # It's a PIL image — convert to NumPy and scale to [0, 1]\n",
    "#         image = np.array(image).astype('float32') / 255.0\n",
    "\n",
    "#     image = np.clip(image, 0, 1)\n",
    "\n",
    "#     fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "#     ax.imshow(image)\n",
    "\n",
    "#     for box in boxes:\n",
    "#         x_min, y_min, x_max, y_max = box\n",
    "#         rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "#                                  linewidth=2, edgecolor='red', facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# transform = CustomTransformation()\n",
    "\n",
    "# print(image_raw, target_raw)\n",
    "\n",
    "# # Test individual transformations manually\n",
    "# image_aug, target_aug = transform.perform_horizontal_flip(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0\n",
    "# )\n",
    "\n",
    "# image_aug3, target_aug3 = transform.perform_vertical_flip(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0\n",
    "# )\n",
    "\n",
    "# image_aug2, target_aug2 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0\n",
    "# )\n",
    "\n",
    "# image_aug4, target_aug4 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0, rotations = [90]\n",
    "# )\n",
    "\n",
    "# image_aug5, target_aug5 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0, rotations = [180]\n",
    "# )\n",
    "\n",
    "# image_aug6, target_aug6 = transform.perform_random_rotation(\n",
    "#     copy.deepcopy(image_raw), {'boxes': target_raw['boxes'].clone()}, prob=1.0, rotations = [270])\n",
    "\n",
    "# # Visualize before/after\n",
    "# visualize(image_raw, target_raw['boxes'], title=\"Original\")\n",
    "# visualize(image_aug, target_aug['boxes'], title=\"Horizontally Flipped\")\n",
    "# visualize(image_aug3, target_aug3['boxes'], title=\"Vertically Flipped\")\n",
    "# visualize(image_aug4, target_aug4['boxes'], title=\"90 degree rotation\")\n",
    "# visualize(image_aug5, target_aug5['boxes'], title=\"180 degree rotation\")\n",
    "# visualize(image_aug6, target_aug6['boxes'], title=\"270 degree rotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8eb2fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created txt for:val_original_1.png\n",
      "Created txt for:val_original_2.png\n",
      "Created txt for:val_original_3.png\n",
      "Created txt for:val_original_4.png\n",
      "Created txt for:val_original_5.png\n",
      "Created txt for:val_original_6.png\n",
      "Created txt for:val_original_7.png\n",
      "Created txt for:val_original_8.png\n",
      "Created txt for:val_original_9.png\n",
      "Created txt for:val_original_10.png\n",
      "Created txt for:val_original_11.png\n",
      "Created txt for:val_original_12.png\n",
      "Created txt for:val_original_13.png\n",
      "Created txt for:val_original_14.png\n",
      "Created txt for:val_original_15.png\n",
      "Created txt for:val_original_16.png\n",
      "Created txt for:val_original_17.png\n",
      "Created txt for:val_original_18.png\n",
      "Created txt for:val_original_19.png\n",
      "Created txt for:val_original_20.png\n",
      "Created txt for:val_original_21.png\n",
      "Created txt for:val_original_22.png\n",
      "Created txt for:val_original_23.png\n",
      "Created txt for:val_original_24.jpg\n",
      "Created txt for:val_original_25.jpg\n",
      "Created txt for:val_original_26.jpg\n",
      "Created txt for:val_original_27.png\n",
      "Created txt for:val_original_28.png\n",
      "Created txt for:val_original_29.png\n",
      "Created txt for:val_original_30.png\n",
      "Created txt for:val_original_31.png\n",
      "Created txt for:val_original_32.png\n",
      "Created txt for:train_original_1.png\n",
      "Created txt for:train_original_2.png\n",
      "Created txt for:train_original_3.png\n",
      "Created txt for:train_original_4.png\n",
      "Created txt for:train_original_5.png\n",
      "Created txt for:train_original_6.png\n",
      "Created txt for:train_original_7.png\n",
      "Created txt for:train_original_8.png\n",
      "Created txt for:train_original_9.png\n",
      "Created txt for:train_original_10.png\n",
      "Created txt for:train_original_11.png\n",
      "Created txt for:train_original_12.png\n",
      "Created txt for:train_original_13.png\n",
      "Created txt for:train_original_14.png\n",
      "Created txt for:train_original_15.png\n",
      "Created txt for:train_original_16.png\n",
      "Created txt for:train_original_17.png\n",
      "Created txt for:train_original_18.png\n",
      "Created txt for:train_original_19.png\n",
      "Created txt for:train_original_20.png\n",
      "Created txt for:train_original_21.png\n",
      "Created txt for:train_original_22.png\n",
      "Created txt for:train_original_23.png\n",
      "Created txt for:train_original_24.png\n",
      "Created txt for:train_original_25.png\n",
      "Created txt for:train_original_26.png\n",
      "Created txt for:train_original_27.png\n",
      "Created txt for:train_original_28.png\n",
      "Created txt for:train_original_29.png\n",
      "Created txt for:train_original_30.png\n",
      "Created txt for:train_original_31.png\n",
      "Created txt for:train_original_32.png\n",
      "Created txt for:train_original_33.png\n",
      "Created txt for:train_original_34.png\n",
      "Created txt for:train_original_35.png\n",
      "Created txt for:train_original_36.png\n",
      "Created txt for:train_original_37.png\n",
      "Created txt for:train_original_38.png\n",
      "Created txt for:train_original_39.png\n",
      "Created txt for:train_original_40.png\n",
      "Created txt for:train_original_41.png\n",
      "Created txt for:train_original_42.png\n",
      "Created txt for:train_original_43.png\n",
      "Created txt for:train_original_44.png\n",
      "Created txt for:train_original_45.png\n",
      "Created txt for:train_original_46.png\n",
      "Created txt for:train_original_47.png\n",
      "Created txt for:train_original_48.png\n",
      "Created txt for:train_original_49.png\n",
      "Created txt for:train_original_50.png\n",
      "Created txt for:train_original_51.png\n",
      "Created txt for:train_original_52.png\n",
      "Created txt for:train_original_53.png\n",
      "Created txt for:train_original_54.png\n",
      "Created txt for:train_original_55.png\n",
      "Created txt for:train_original_56.png\n",
      "Created txt for:train_original_57.png\n",
      "Created txt for:train_original_58.png\n",
      "Created txt for:train_original_59.png\n",
      "Created txt for:train_original_60.png\n",
      "Created txt for:train_original_61.png\n",
      "Created txt for:train_original_62.png\n",
      "Created txt for:train_original_63.png\n",
      "Created txt for:train_original_64.png\n",
      "Created txt for:train_original_65.png\n",
      "Created txt for:train_original_66.png\n",
      "Created txt for:train_original_67.png\n",
      "Created txt for:train_original_68.png\n",
      "Created txt for:train_original_69.png\n",
      "Created txt for:train_original_70.png\n",
      "Created txt for:train_original_71.png\n",
      "Created txt for:train_original_72.png\n",
      "Created txt for:train_original_73.png\n",
      "Created txt for:train_original_74.png\n",
      "Created txt for:train_original_75.png\n",
      "Created txt for:train_original_76.png\n",
      "Created txt for:train_original_77.png\n",
      "Created txt for:train_original_78.png\n",
      "Created txt for:train_original_79.png\n",
      "Created txt for:train_original_80.png\n",
      "Created txt for:train_original_81.png\n",
      "Created txt for:train_original_82.png\n",
      "Created txt for:train_original_83.png\n",
      "Created txt for:train_original_84.png\n",
      "Created txt for:train_original_85.png\n",
      "Created txt for:train_original_86.png\n",
      "Created txt for:train_original_87.png\n",
      "Created txt for:train_original_88.png\n",
      "Created txt for:train_original_89.png\n",
      "Created txt for:train_original_90.png\n",
      "Created txt for:train_original_91.png\n",
      "Created txt for:train_original_92.png\n",
      "Created txt for:train_original_93.png\n",
      "Created txt for:train_original_94.png\n",
      "Created txt for:train_original_95.png\n",
      "Created txt for:train_original_96.png\n",
      "Created txt for:train_original_97.png\n",
      "Created txt for:train_original_98.png\n",
      "Created txt for:train_original_99.png\n",
      "Created txt for:train_original_100.png\n",
      "Created txt for:train_original_101.png\n",
      "Created txt for:train_original_102.png\n",
      "Created txt for:train_original_103.png\n",
      "Created txt for:train_original_104.png\n",
      "Created txt for:train_original_105.png\n",
      "Created txt for:train_original_106.png\n",
      "Created txt for:train_original_107.png\n",
      "Created txt for:train_original_108.png\n",
      "Created txt for:train_original_109.png\n",
      "Created txt for:train_original_110.png\n",
      "Created txt for:train_original_111.png\n",
      "Created txt for:train_original_112.png\n",
      "Created txt for:train_original_113.png\n",
      "Created txt for:train_original_114.png\n",
      "Created txt for:train_original_115.png\n",
      "Created txt for:train_original_116.png\n",
      "Created txt for:train_original_117.png\n",
      "Created txt for:train_original_118.png\n",
      "Created txt for:train_original_119.png\n",
      "Created txt for:train_original_120.png\n",
      "Created txt for:train_original_121.png\n",
      "Created txt for:train_original_122.png\n",
      "Created txt for:train_original_123.png\n",
      "Created txt for:train_original_124.png\n",
      "Created txt for:train_original_125.png\n",
      "Created txt for:train_original_126.png\n",
      "Created txt for:train_original_127.png\n",
      "Created txt for:train_original_128.png\n",
      "Created txt for:train_original_129.png\n",
      "Created txt for:train_original_130.png\n",
      "Created txt for:train_original_131.png\n",
      "Created txt for:train_original_132.png\n",
      "Created txt for:train_original_133.png\n",
      "Created txt for:train_original_134.png\n",
      "Created txt for:train_original_135.png\n",
      "Created txt for:train_original_136.png\n",
      "Created txt for:train_original_137.png\n",
      "Created txt for:train_original_138.png\n",
      "Created txt for:train_original_139.png\n",
      "Created txt for:train_original_140.png\n",
      "Created txt for:train_original_141.png\n",
      "Created txt for:train_original_142.png\n",
      "Created txt for:train_original_143.png\n",
      "Created txt for:train_original_144.png\n",
      "Created txt for:train_original_145.png\n",
      "Created txt for:train_original_146.png\n",
      "Created txt for:train_original_147.png\n",
      "Created txt for:train_original_148.png\n",
      "Created txt for:train_original_149.png\n",
      "Created txt for:train_original_150.png\n",
      "Created txt for:train_original_151.png\n",
      "Created txt for:train_original_152.png\n",
      "Created txt for:train_original_153.png\n",
      "Created txt for:train_original_154.png\n",
      "Created txt for:train_original_155.png\n",
      "Created txt for:train_original_156.png\n",
      "Created txt for:train_original_157.png\n",
      "Created txt for:train_original_158.png\n",
      "Created txt for:train_original_159.png\n",
      "Created txt for:train_original_160.png\n",
      "Created txt for:train_original_161.png\n",
      "Created txt for:train_original_162.png\n",
      "Created txt for:train_original_163.png\n",
      "Created txt for:train_original_164.png\n",
      "Created txt for:train_original_165.png\n",
      "Created txt for:train_original_166.png\n",
      "Created txt for:train_original_167.png\n",
      "Created txt for:train_original_168.png\n",
      "Created txt for:train_original_169.png\n",
      "Created txt for:train_original_170.png\n",
      "Created txt for:train_original_171.png\n",
      "Created txt for:train_original_172.png\n",
      "Created txt for:train_original_173.png\n",
      "Created txt for:train_original_174.png\n",
      "Created txt for:train_original_175.png\n",
      "Created txt for:train_original_176.png\n",
      "Created txt for:train_original_177.png\n",
      "Created txt for:train_original_178.png\n",
      "Created txt for:train_original_179.png\n",
      "Created txt for:train_original_180.png\n",
      "Created txt for:train_original_181.png\n",
      "Created txt for:train_original_182.jpg\n",
      "Created txt for:train_original_183.jpg\n",
      "Created txt for:train_original_184.jpg\n",
      "Created txt for:train_original_185.jpg\n",
      "Created txt for:train_original_186.png\n",
      "Created txt for:train_original_187.jpg\n",
      "Created txt for:train_original_188.jpg\n",
      "Created txt for:train_original_189.png\n",
      "Created txt for:train_original_190.jpg\n",
      "Created txt for:train_original_191.jpg\n",
      "Created txt for:train_original_192.jpg\n",
      "Created txt for:train_original_193.jpg\n",
      "Created txt for:train_original_194.jpg\n",
      "Created txt for:train_original_195.jpg\n",
      "Created txt for:train_original_196.jpg\n",
      "Created txt for:train_original_197.jpg\n",
      "Created txt for:train_original_198.png\n",
      "Created txt for:train_original_199.jpg\n",
      "Created txt for:train_original_200.jpg\n",
      "Created txt for:train_original_201.jpg\n",
      "Created txt for:train_original_202.jpg\n",
      "Created txt for:train_original_203.jpg\n",
      "Created txt for:train_original_204.png\n",
      "Created txt for:train_original_205.jpg\n",
      "Created txt for:train_original_206.jpg\n",
      "Created txt for:train_original_207.jpg\n",
      "Created txt for:train_original_208.png\n",
      "Created txt for:train_original_209.png\n",
      "Created txt for:train_original_210.png\n",
      "Created txt for:train_original_211.png\n",
      "Created txt for:train_original_212.png\n",
      "Created txt for:train_original_213.png\n",
      "Created txt for:train_original_214.png\n",
      "Created txt for:train_original_215.png\n",
      "Created txt for:train_original_216.png\n",
      "Created txt for:train_original_217.png\n",
      "Created txt for:train_original_218.png\n",
      "Created txt for:train_original_219.png\n",
      "Created txt for:train_original_220.png\n",
      "Created txt for:train_original_221.png\n",
      "Created txt for:train_original_222.png\n",
      "Created txt for:train_original_223.png\n",
      "Created txt for:train_original_224.png\n",
      "Created txt for:train_original_225.png\n",
      "Created txt for:train_original_226.png\n",
      "Created txt for:train_original_227.png\n",
      "Created txt for:train_original_228.png\n",
      "Created txt for:train_original_229.png\n",
      "Created txt for:train_original_230.png\n",
      "Created txt for:train_original_231.png\n",
      "Created txt for:train_original_232.png\n",
      "Created txt for:train_original_233.png\n",
      "Created txt for:train_original_234.png\n",
      "Created txt for:train_original_235.png\n",
      "Created txt for:train_original_236.png\n",
      "Created txt for:train_original_237.png\n",
      "Created txt for:train_original_238.png\n",
      "Created txt for:train_original_239.png\n",
      "Created txt for:train_original_240.png\n",
      "Created txt for:train_original_241.png\n",
      "Created txt for:train_original_242.png\n",
      "Created txt for:train_original_243.png\n",
      "Created txt for:train_original_244.png\n",
      "Created txt for:train_original_245.png\n",
      "Created txt for:train_original_246.png\n",
      "Created txt for:train_original_247.png\n",
      "Created txt for:train_original_248.png\n",
      "Created txt for:train_original_249.png\n",
      "Created txt for:train_original_250.png\n",
      "Created txt for:train_original_251.png\n",
      "Created txt for:train_original_252.png\n",
      "Created txt for:train_original_253.png\n",
      "Created txt for:train_original_254.png\n",
      "Created txt for:train_original_255.png\n",
      "Created txt for:train_original_256.png\n",
      "Created txt for:train_original_257.png\n",
      "Created txt for:train_original_258.png\n",
      "Created txt for:train_original_259.png\n",
      "Created txt for:train_original_260.png\n",
      "Created txt for:train_original_261.png\n",
      "Created txt for:train_original_262.png\n",
      "Created txt for:train_original_263.png\n",
      "Created txt for:test_original_1.png\n",
      "Created txt for:test_original_2.png\n",
      "Created txt for:test_original_3.png\n",
      "Created txt for:test_original_4.png\n",
      "Created txt for:test_original_5.png\n",
      "Created txt for:test_original_6.png\n",
      "Created txt for:test_original_7.png\n",
      "Created txt for:test_original_8.png\n",
      "Created txt for:test_original_9.png\n",
      "Created txt for:test_original_10.png\n",
      "Created txt for:test_original_11.png\n",
      "Created txt for:test_original_12.png\n",
      "Created txt for:test_original_13.png\n",
      "Created txt for:test_original_14.png\n",
      "Created txt for:test_original_15.png\n",
      "Created txt for:test_original_16.png\n",
      "Created txt for:test_original_17.png\n",
      "Created txt for:test_original_18.png\n",
      "Created txt for:test_original_19.png\n",
      "Created txt for:test_original_20.png\n",
      "Created txt for:test_original_21.png\n",
      "Created txt for:test_original_22.png\n",
      "Created txt for:test_original_23.jpg\n",
      "Created txt for:test_original_24.jpg\n",
      "Created txt for:test_original_25.jpg\n",
      "Created txt for:test_original_26.png\n",
      "Created txt for:test_original_27.png\n",
      "Created txt for:test_original_28.png\n",
      "Created txt for:test_original_29.png\n",
      "Created txt for:test_original_30.png\n",
      "Created txt for:test_original_31.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "#this changes the annations format to Txt which Yolo can read. \n",
    "def convert_to_yolo_format(data_path, annotations_file, output_dir):\n",
    "    # Load annotations JSON file\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations_list = json.load(f)\n",
    "\n",
    "    # Ensure output directories exist\n",
    "        os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "\n",
    "    #dit moest blijkbaar\n",
    "    class_map = {\"Bird\": 0}\n",
    "\n",
    "    # ocess each image\n",
    "    for entry in annotations_list:\n",
    "        image_name = entry['OriginalFileName']\n",
    "        annotation_data = entry['AnnotationData']\n",
    "\n",
    "\n",
    "        # Load image to get width and height\n",
    "        image_path = os.path.join(data_path, 'images', image_name)\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            #print(f\"Image size: {img_width} x {img_height}\")\n",
    "\n",
    "        # Create the label file for this image\n",
    "        label_file = os.path.join(output_dir, 'labels', os.path.splitext(image_name)[0] + '.txt')\n",
    "\n",
    "        with open(label_file, 'w') as label_f:\n",
    "            for obj in annotation_data:\n",
    "                class_name = obj['Label']  # 'Label' field in your data\n",
    "                \n",
    "                if class_name in class_map:\n",
    "                    # Get the coordinates (bounding box)\n",
    "                    coordinates = obj['Coordinates']\n",
    "                    \n",
    "                    # Calculate bounding box (x_min, y_min, width, height)\n",
    "                    x_min = min([coord['X'] for coord in coordinates])\n",
    "                    y_min = min([coord['Y'] for coord in coordinates])\n",
    "                    x_max = max([coord['X'] for coord in coordinates])\n",
    "                    y_max = max([coord['Y'] for coord in coordinates])\n",
    "\n",
    "                    # YOLO format: class_id x_center y_center width height (all normalized)\n",
    "                    class_id = class_map[class_name]\n",
    "                    x_center = (x_min + x_max) / 2 / img_width\n",
    "                    y_center = (y_min + y_max) / 2 / img_height\n",
    "                    norm_width = (x_max - x_min) / img_width\n",
    "                    norm_height = (y_max - y_min) / img_height\n",
    "                    \n",
    "                    # Write the YOLO annotation to the label file\n",
    "                    label_f.write(f\"{class_id} {x_center} {y_center} {norm_width} {norm_height}\\n\")\n",
    "                    \n",
    "           \n",
    "        print(\"Created txt for:\"+image_name)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "data_path = \"datasets/scarecrow_dataset/val\"  # Path to the train folder\n",
    "annotations_file = \"datasets/scarecrow_dataset/val/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"datasets/scarecrow_dataset/val\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n",
    "data_path = \"datasets/scarecrow_dataset/train\"  # Path to the train folder\n",
    "annotations_file = \"datasets/scarecrow_dataset/train/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"datasets/scarecrow_dataset/train\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n",
    "data_path = \"datasets/scarecrow_dataset/test\"  # Path to the train folder\n",
    "annotations_file = \"datasets/scarecrow_dataset/test/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"datasets/scarecrow_dataset/test\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d90992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.128  Python-3.12.7 torch-2.6.0+cpu CPU (11th Gen Intel Core(TM) i7-1195G7 2.90GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=c:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\runs\\detect\\train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\thomz\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755k/755k [00:00<00:00, 12.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 601.177.4 MB/s, size: 10263.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\datasets\\scarecrow_dataset\\train\\labels... 263 images, 20 backgrounds, 0 corrupt: 100%|██████████| 263/263 [00:10<00:00, 24.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\datasets\\scarecrow_dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.5 ms, read: 482.753.3 MB/s, size: 7541.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\datasets\\scarecrow_dataset\\val\\labels... 32 images, 5 backgrounds, 0 corrupt: 100%|██████████| 32/32 [00:01<00:00, 24.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mC:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\datasets\\scarecrow_dataset\\val\\images\\DJI_0319.JPG: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\datasets\\scarecrow_dataset\\val\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to c:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\runs\\detect\\train4\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\angry_birds\\DL_angrybirds\\runs\\detect\\train4\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      2.674      8.067      1.044        212        640:  56%|█████▌    | 5/9 [01:46<01:25, 21.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Load YOLO model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Example: Add dropout to YOLO layers\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Path to dataset YAML file\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# Set to 0 for GPU, 'cpu' for CPU\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the validation set\u001b[39;00m\n\u001b[0;32m     15\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:793\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:212\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:368\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    366\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), total\u001b[38;5;241m=\u001b[39mnb)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_batch_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Warmup\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\data\\build.py:59\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\data\\base.py:376\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    375\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return transformed label information for given index.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_and_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\data\\base.py:390\u001b[0m, in \u001b[0;36mBaseDataset.get_image_and_label\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    388\u001b[0m label \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index])  \u001b[38;5;66;03m# requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\u001b[39;00m\n\u001b[0;32m    389\u001b[0m label\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# shape is for rect, remove it\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m], label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mori_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m], label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresized_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratio_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    392\u001b[0m     label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresized_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mori_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    393\u001b[0m     label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresized_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m label[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mori_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    394\u001b[0m )  \u001b[38;5;66;03m# for evaluation\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrect:\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\data\\base.py:233\u001b[0m, in \u001b[0;36mBaseDataset.load_image\u001b[1;34m(self, i, rect_mode)\u001b[0m\n\u001b[0;32m    231\u001b[0m         im \u001b[38;5;241m=\u001b[39m imread(f, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2_flag)  \u001b[38;5;66;03m# BGR\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# read image\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2_flag\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# BGR\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage Not Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thomz\\Documents\\JADS\\Master\\Deep Learning\\DL_angrybirds\\venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py:30\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Read an image from a file.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m        >>> img = imread(\"path/to/image.jpg\", cv2.IMREAD_GRAYSCALE)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     file_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tiff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     32\u001b[0m         success, frames \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimdecodemulti(file_bytes, cv2\u001b[38;5;241m.\u001b[39mIMREAD_UNCHANGED)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")  # Load YOLO model\n",
    "# Example: Add dropout to YOLO layers\n",
    "\n",
    "model.train(\n",
    "    data='data.yaml', # Path to dataset YAML file\n",
    "    epochs=50,                          \n",
    "    imgsz=640,                          \n",
    "    batch=32,                           \n",
    "    device='cpu'                             # Set to 0 for GPU, 'cpu' for CPU\n",
    ")\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(\"datasets/scarecrow_dataset/test/images/20240901120856_0268_D_frame_1020 - kopie.png\")  # Predict on an image\n",
    "results[0].show()  # Display results\n",
    "#model.save('yolov8_trained.pt') # saved yolov8s model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ecd89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'YOLOModel' object has no attribute 'train_loop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m model_utils\u001b[38;5;241m.\u001b[39mYOLOModel(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Use 'cuda' if you want to use a GPU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model (with your DataLoader for training and validation)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m(train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# After training, test the model on the test dataset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(test_loader)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'YOLOModel' object has no attribute 'train_loop'"
     ]
    }
   ],
   "source": [
    "import model_utils\n",
    "import importlib\n",
    "importlib.reload(model_utils)\n",
    "\n",
    "# Initialize YOLOTrainer with your model path and device (e.g., 'cpu' or 'cuda')\n",
    "trainer = model_utils.YOLOModel(model_path='yolov8n.pt', device='cpu')  # Use 'cuda' if you want to use a GPU\n",
    "\n",
    "# Train the model (with your DataLoader for training and validation)\n",
    "trainer.train(data_yaml='data.yaml', epochs=10, imgsz=640, batch_size=16)\n",
    "\n",
    "\n",
    "# After training, test the model on the test dataset\n",
    "test_accuracy = trainer.test(test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This checks all the hyperparameters and finds the best one\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "batch_sizes = [32]\n",
    "optimizers = ['SGD', 'Adam']\n",
    "momentums = [0.9, 0.99]  # Only used for SGD\n",
    "weight_decays = [1e-6, 1e-5, 1e-4]\n",
    "image_sizes = [480, 640]\n",
    "\n",
    "# Initialize YOLO model\n",
    "model_path = 'yolo11n.pt'\n",
    "data_path = './data.yaml'\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "best_map = 0\n",
    "best_params = None\n",
    "\n",
    "for lr, batch_size, optimizer, img_size, weight_decay in itertools.product(\n",
    "    learning_rates, batch_sizes, optimizers, image_sizes, weight_decays\n",
    "):\n",
    "    # Set momentum only for SGD\n",
    "    momentum = 0.9 if optimizer == 'SGD' else None\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        epochs=15,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        device=0,\n",
    "        lr0=lr,\n",
    "        optimizer=optimizer,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Get the metric to optimize (e.g., mAP@50)\n",
    "    map50 = results.maps\n",
    "\n",
    "    # Log the results\n",
    "    print(f\"lr: {lr}, batch_size: {batch_size}, optimizer: {optimizer}, img_size: {img_size}, weight_decay: {weight_decay}, mAP@50: {map50}\")\n",
    "\n",
    "    # Update the best parameters\n",
    "    if map50 > best_map:\n",
    "        best_map = map50\n",
    "        best_params = {\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'optimizer': optimizer,\n",
    "            'momentum': momentum,\n",
    "            'weight_decay': weight_decay,\n",
    "            'img_size': img_size\n",
    "        }\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best mAP@50:\", best_map)\n",
    "#Application of regularization techniques (e.g., dropout, batch normalization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is there to check if the Annotations are working correctly.\n",
    "def visualize_yolo_annotations(image_path, label_path, class_names=None):\n",
    " \n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_width, img_height = image.size\n",
    "\n",
    "    # Create a plot\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Read the YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse each line in the label file\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])\n",
    "        x_center, y_center, width, height = map(float, parts[1:])\n",
    "\n",
    "        # Convert normalized coordinates to absolute pixel values\n",
    "        x_center *= img_width\n",
    "        y_center *= img_height\n",
    "        width *= img_width\n",
    "        height *= img_height\n",
    "\n",
    "        # Calculate the top-left corner of the bounding box\n",
    "        x_min = x_center - (width / 2)\n",
    "        y_min = y_center - (height / 2)\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min), width, height,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label (if provided)\n",
    "        if class_names:\n",
    "            ax.text(\n",
    "                x_min, y_min - 5, class_names[class_id],\n",
    "                color='red', fontsize=12, backgroundcolor='white'\n",
    "            )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = \"scarecrow_dataset/train/images/train_original_89.png\"\n",
    "label_path = \"scarecrow_dataset/train/labels/train_original_89.txt\"\n",
    "class_names = [\"Bird\"]  #\n",
    "\n",
    "visualize_yolo_annotations(image_path, label_path, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
