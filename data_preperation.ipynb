{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f9751c",
   "metadata": {},
   "source": [
    "# Deep Learning group assignment\n",
    "Group name: Angry Birds\n",
    "\n",
    "Group members:\n",
    "- Nienke Reijnen: 2117034\n",
    "- Andrea Ciavatti: 2115635\n",
    "- Niels Boonstra: 1451294\n",
    "- Yannick Lankhorst: 2052754\n",
    "- Thom Zoomer:2059225\n",
    "- Anne Barnasconi: 2053988"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc615d47",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1df5d",
   "metadata": {},
   "source": [
    "Before running, make sure to also have installed the following packages (according to lab 8 instructions):\n",
    "- pip install imageio\n",
    "- pip install future\n",
    "- pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b751e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchvision.transforms import functional\n",
    "from torch.utils.data import ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import itertools\n",
    "from ultralytics import YOLO\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, device_name = (torch.device(\"cuda\"), torch.cuda.get_device_name(0)) if torch.cuda.is_available() else (torch.device(\"cpu\"), \"CPU\")\n",
    "print(f\"Device: {device}, {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f9ca2",
   "metadata": {},
   "source": [
    "## Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0c073",
   "metadata": {},
   "source": [
    "- DONE Correct implementation of data loaders for images and annotations for your specific object detection model\n",
    "- DONE Use of data augmentation techniques\n",
    "- DONE Appropriate shuffling  and batching of data\n",
    "- TO DO: Conduct an online search for relevant open-source datasets, and if you can find them, use them in your application as additional training data (to improve generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999552a",
   "metadata": {},
   "source": [
    "### The code below needs to be run on your own laptop to convert the names of the scare_crow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory and dataset splits\n",
    "base_dir = \"scarecrow_dataset\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Allowed image extensions (all lowercased for matching)\n",
    "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\nProcessing {split} split...\")\n",
    "\n",
    "    image_dir = os.path.join(base_dir, split, \"images\")\n",
    "    annotation_path = os.path.join(base_dir, split, \"annotations.json\")\n",
    "\n",
    "    # Load annotations\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    print(f\"Found {len(annotations)} annotation entries in annotations.json\")\n",
    "\n",
    "    # Loop through annotations and rename corresponding images\n",
    "    for idx, annotation in enumerate(annotations, 1):\n",
    "        old_name = annotation[\"OriginalFileName\"]\n",
    "        ext = os.path.splitext(old_name)[1].lower()  # Preserve the file extension\n",
    "        new_name = f\"{split}_original_{idx}{ext}\"\n",
    "\n",
    "        old_path = os.path.join(image_dir, old_name)\n",
    "        new_path = os.path.join(image_dir, new_name)\n",
    "\n",
    "        # Check if the image file exists before renaming\n",
    "        if os.path.exists(old_path):\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed {old_name} to {new_name}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Image file {old_name} not found in {image_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Update the annotation with the new image name\n",
    "        annotation[\"OriginalFileName\"] = new_name\n",
    "\n",
    "    # Save updated annotations\n",
    "    with open(annotation_path, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)\n",
    "\n",
    "    print(f\"{split} renamed and annotations updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9182d00",
   "metadata": {},
   "source": [
    "### Data loading & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8585276",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Defining a CustomDataset class ###\n",
    "######################################\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform = None):\n",
    "        \"\"\"\n",
    "        Initialize the custom dataset.\n",
    "        Works for both the train data and the test data.\n",
    "        \"\"\"\n",
    "        self.images_dir = os.path.join(data_path, \"images\")\n",
    "        self.transform = transform\n",
    "        annotations_file = data_path + \"/annotations.json\"\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            annotations_list = json.load(f)\n",
    "       \n",
    "        # We need to extract the bounding boxes of the annotations from the JSON file and store them as [x_min, y_min, x_max, y_max] tensors\n",
    "        self.data = []\n",
    "        for entry in annotations_list:\n",
    "            image_name = entry['OriginalFileName']\n",
    "            annotation_data = entry['AnnotationData']\n",
    "            bird_boxes = self.extract_bird_boxes(annotation_data)\n",
    "            self.data.append({'imagename': image_name, 'bird_boxes_tensor': bird_boxes})\n",
    "\n",
    "        # Note: we should not load all the images into a tensor here, as it would take too much memory. We load images into a tensor in the __getitem__ method.\n",
    "\n",
    "\n",
    "    def extract_bird_boxes(self, annotation_data):\n",
    "        \"\"\"\n",
    "        Extract the coordinates of the birds from the annotation data in the JSON file and return it as a tensor.\n",
    "        \"\"\"\n",
    "        bird_boxes = []\n",
    "        for entry in annotation_data:\n",
    "            if entry['Label'] == 'Bird':\n",
    "                coordinates_list = entry['Coordinates']\n",
    "                x_coordinates = [point['X'] for point in coordinates_list]\n",
    "                y_coordinates = [point['Y'] for point in coordinates_list]\n",
    "                x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
    "                y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
    "                bird_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return torch.tensor(bird_boxes, dtype=torch.float32) # Shape: (num_birds, 4)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset, i.e. the number of images.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load an image and its corresponding annotations.\n",
    "        Returns the image and a target dictionary with bounding boxes and labels (we need this for compatiblity with object detection models like Faster R-CNN)\n",
    "        \"\"\"\n",
    "        item = self.data[index]\n",
    "        image_path = os.path.join(self.images_dir, item['imagename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        bird_boxes = item['bird_boxes_tensor']\n",
    "        labels = torch.ones((bird_boxes.shape[0],), dtype=torch.int64) # Assuming all the labels are 'Bird' --> we assign this to class 1\n",
    "        target = {'boxes': bird_boxes, 'labels': labels} # should contain the bounding boxes and the labels\n",
    "\n",
    "        # Apply data augmentations\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### Finding the mean and std of the dataset ###\n",
    "###############################################\n",
    "\n",
    "def calculate_mean_and_std(dataset):\n",
    "    # Initialize sums for mean and variance\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for image, _ in tqdm(dataset, desc=\"Calculating Mean and Std\", unit=\"image\"):\n",
    "        # Convert image to tensor if it is in PIL format\n",
    "        image = transforms.ToTensor()(image)  # shape: (C, H, W)\n",
    "        \n",
    "        # Calculate the sum and squared sum of pixels for each channel\n",
    "        mean += image.mean([1, 2])  # mean per channel (C,)\n",
    "        std += image.std([1, 2])    # std per channel (C,)\n",
    "        num_pixels += 1\n",
    "    \n",
    "    # Average the sums to get the mean and std\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# For now, do no transformations:\n",
    "train_data_original = CustomDataset(\"scarecrow_dataset/train\", transform=None)\n",
    "train_data_extra = CustomDataset(\"bird-detection-farm/train\", transform=None)\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "\n",
    "mean, std = calculate_mean_and_std(train_data)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165af7f",
   "metadata": {},
   "source": [
    "Dataset Mean: tensor([0.5390, 0.5306, 0.4421])\n",
    "\n",
    "Dataset Std: tensor([0.1624, 0.1527, 0.1647])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c97e9b",
   "metadata": {},
   "source": [
    "### With the extra data\n",
    "\n",
    "Dataset Mean: tensor([0.5409, 0.5505, 0.3894])\n",
    "\n",
    "Dataset Std: tensor([0.1674, 0.1557, 0.1689])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Defining a CustomTransformation class ###\n",
    "##############################################\n",
    "\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "mean = [0.5409, 0.5505, 0.3894]\n",
    "std = [0.1674, 0.1557, 0.1689]\n",
    "\n",
    "class CustomTransformation:\n",
    "    def __init__(self):\n",
    "        self.transforms = transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean, std)])\n",
    "        \n",
    "\n",
    "    def perform_horizontal_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Horizontally flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.hflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            width, _ = image.size\n",
    "            x_min = boxes[:, 0].clone()\n",
    "            x_max = boxes[:, 2].clone()\n",
    "            boxes[:, 0] = width - x_max\n",
    "            boxes[:, 2] = width - x_min\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_vertical_flip(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Vertically flips the image with a given probability, default is 0.25\n",
    "        \"\"\"\n",
    "        flip_prob = prob\n",
    "        if random.random() < flip_prob:\n",
    "            # Flip the image\n",
    "            image = functional.vflip(image)\n",
    "\n",
    "            # Flip the bounding boxes\n",
    "            boxes = target['boxes']\n",
    "            _, height = image.size\n",
    "            y_min = boxes[:, 1].clone()\n",
    "            y_max = boxes[:, 3].clone()\n",
    "            boxes[:, 1] = height - y_max\n",
    "            boxes[:, 3] = height - y_min\n",
    "            target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "    def perform_random_rotation(self, image, target, prob = 0.25):\n",
    "        \"\"\"\n",
    "        Perform a random rotation in multiples of 90 degrees with a given probability, default is 0.25.\n",
    "        \"\"\"\n",
    "        if random.random() < prob:\n",
    "            angle = random.choice([90, 180, 270])\n",
    "\n",
    "            image = functional.rotate(image, angle)\n",
    "\n",
    "            boxes = target['boxes']\n",
    "            width, height = image.size\n",
    "\n",
    "            if angle == 90:\n",
    "                boxes = boxes[:, [1, 0, 3, 2]] # swap x and y coordinates\n",
    "                boxes[:, [0, 2]] = height - boxes[:, [2, 0]] # adjut x-coordinates because the rotation changes the origin\n",
    "            elif angle == 180:\n",
    "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]] # adjust x-coordinates\n",
    "                boxes[:, [1, 3]] = height - boxes[:, [3, 1]] # adjust y-coordinates\n",
    "            elif angle == 270:\n",
    "                boxes = boxes[:, [1, 0, 3, 2]] # swap x and y coordinates\n",
    "                boxes[:, [1, 3]] = width - boxes[:, [3, 1]] # adjust y-coordinates because the rotation changes to origin\n",
    "            target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def perform_random_resize(self, image, target, scale_range=(0.75, 1.25)):\n",
    "        \"\"\"\n",
    "        Perform a random reize within the specified scale range, default scale range is (0,75, 1.25)\n",
    "        \"\"\"\n",
    "        scale = random.uniform(*scale_range)\n",
    "\n",
    "        # Resize the image\n",
    "        width, height = image.size\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        image = functional.resize(image, [new_height, new_width])\n",
    "        \n",
    "        # Resize the boxes\n",
    "        boxes = target['boxes']\n",
    "        boxes = boxes * scale\n",
    "        target['boxes'] = boxes\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        Apply the transformations to an image\n",
    "        \"\"\"\n",
    "        image, target = self.perform_horizontal_flip(image, target)\n",
    "        image, target = self.perform_vertical_flip(image, target)\n",
    "        image, target = self.perform_random_rotation(image, target)\n",
    "        image, target = self.perform_random_resize(image, target)\n",
    "        image = self.transforms(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Define paths\n",
    "train_data_original_path = \"scarecrow_dataset/train\"\n",
    "test_data_original_path = \"scarecrow_dataset/test\"\n",
    "val_data_original_path = \"scarecrow_dataset/val\"\n",
    "train_data_extra_path = \"bird-detection-farm/train\"\n",
    "valid_data_extra_path = \"bird-detection-farm/valid\"\n",
    "test_data_extra_path = \"bird-detection-farm/test\"\n",
    "\n",
    "# Loading the datasets with the transformations\n",
    "transform = CustomTransformation()\n",
    "\n",
    "train_data_original = CustomDataset(train_data_original_path, transform)\n",
    "valid_data_original = CustomDataset(val_data_original_path, transform)\n",
    "test_data_original = CustomDataset(test_data_original_path, transform)\n",
    "\n",
    "train_data_extra = CustomDataset(train_data_extra_path, transform)\n",
    "valid_data_extra = CustomDataset(valid_data_extra_path, transform)\n",
    "test_data_extra = CustomDataset(test_data_extra_path, transform)\n",
    "\n",
    "# Split old training set into train/val\n",
    "#train_data_original, valid_data_original = torch.utils.data.random_split(train_data_original, [0.8, 0.2])\n",
    "\n",
    "# Combine datasets\n",
    "train_data = torch.utils.data.ConcatDataset([train_data_original, train_data_extra])\n",
    "valid_data = torch.utils.data.ConcatDataset([valid_data_original, valid_data_extra])\n",
    "test_data = torch.utils.data.ConcatDataset([test_data_original, test_data_extra])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# No shuffling for validation and test data because we want consistnt order for reproducibility:\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one image and its corresponding target from the dataset\n",
    "image, target = train_data_original[88]  # Replace 0 with any index to fetch a different image\n",
    "\n",
    "# Convert the image tensor to a NumPy array for visualization\n",
    "image_np = image.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "image_np = (image_np * std + mean).clip(0, 1)  # Denormalize the image\n",
    "\n",
    "# Visualize the image with bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image_np)\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in target['boxes']:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    rect = patches.Rectangle(\n",
    "        (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "        linewidth=2, edgecolor='r', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this changes the annations format to Txt which Yolo can read. \n",
    "def convert_to_yolo_format(data_path, annotations_file, output_dir):\n",
    "    # Load annotations JSON file\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations_list = json.load(f)\n",
    "\n",
    "    # Ensure output directories exist\n",
    "        os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "\n",
    "    #dit moest blijkbaar\n",
    "    class_map = {\"Bird\": 0}\n",
    "\n",
    "    # ocess each image\n",
    "    for entry in annotations_list:\n",
    "        image_name = entry['OriginalFileName']\n",
    "        annotation_data = entry['AnnotationData']\n",
    "\n",
    "\n",
    "        # Load image to get width and height\n",
    "        image_path = os.path.join(data_path, 'images', image_name)\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            #print(f\"Image size: {img_width} x {img_height}\")\n",
    "\n",
    "        # Create the label file for this image\n",
    "        label_file = os.path.join(output_dir, 'labels', os.path.splitext(image_name)[0] + '.txt')\n",
    "\n",
    "        with open(label_file, 'w') as label_f:\n",
    "            for obj in annotation_data:\n",
    "                class_name = obj['Label']  # 'Label' field in your data\n",
    "                \n",
    "                if class_name in class_map:\n",
    "                    # Get the coordinates (bounding box)\n",
    "                    coordinates = obj['Coordinates']\n",
    "                    \n",
    "                    # Calculate bounding box (x_min, y_min, width, height)\n",
    "                    x_min = min([coord['X'] for coord in coordinates])\n",
    "                    y_min = min([coord['Y'] for coord in coordinates])\n",
    "                    x_max = max([coord['X'] for coord in coordinates])\n",
    "                    y_max = max([coord['Y'] for coord in coordinates])\n",
    "\n",
    "                    # YOLO format: class_id x_center y_center width height (all normalized)\n",
    "                    class_id = class_map[class_name]\n",
    "                    x_center = (x_min + x_max) / 2 / img_width\n",
    "                    y_center = (y_min + y_max) / 2 / img_height\n",
    "                    norm_width = (x_max - x_min) / img_width\n",
    "                    norm_height = (y_max - y_min) / img_height\n",
    "                    \n",
    "                    # Write the YOLO annotation to the label file\n",
    "                    label_f.write(f\"{class_id} {x_center} {y_center} {norm_width} {norm_height}\\n\")\n",
    "                    \n",
    "           \n",
    "        print(\"Created txt for:\"+image_name)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "data_path = \"scarecrow_dataset/val\"  # Path to the train folder\n",
    "annotations_file = \"scarecrow_dataset/val/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"scarecrow_dataset/val\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n",
    "data_path = \"scarecrow_dataset/train\"  # Path to the train folder\n",
    "annotations_file = \"scarecrow_dataset/train/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"scarecrow_dataset/train\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n",
    "data_path = \"scarecrow_dataset/test\"  # Path to the train folder\n",
    "annotations_file = \"scarecrow_dataset/test/annotations.json\"  # Path to annotations.json\n",
    "output_dir = \"scarecrow_dataset/test\"  # Output directory for YOLO annotations\n",
    "convert_to_yolo_format(data_path, annotations_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d90992",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")  # Load YOLO model\n",
    "# Example: Add dropout to YOLO layers\n",
    "\n",
    "model.train(\n",
    "    data='C:/Users/niels/.vscode/Deeplearning/angryBirds/DL_angrybirds/data.yaml', # Path to dataset YAML file\n",
    "    epochs=50,                          \n",
    "    imgsz=640,                          \n",
    "    batch=32,                           \n",
    "    device=0                             # Set to 0 for GPU, 'cpu' for CPU\n",
    ")\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(\"scarecrow_dataset/test/images/20240901120856_0268_D_frame_1020 - kopie.png\")  # Predict on an image\n",
    "results[0].show()  # Display results\n",
    "#model.save('yolov8_trained.pt') # saved yolov8s model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This checks all the hyperparameters and finds the best one\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "batch_sizes = [32]\n",
    "optimizers = ['SGD', 'Adam']\n",
    "momentums = [0.9, 0.99]  # Only used for SGD\n",
    "weight_decays = [1e-6, 1e-5, 1e-4]\n",
    "image_sizes = [480, 640]\n",
    "\n",
    "# Initialize YOLO model\n",
    "model_path = 'yolo11n.pt'\n",
    "data_path = './data.yaml'\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "best_map = 0\n",
    "best_params = None\n",
    "\n",
    "for lr, batch_size, optimizer, img_size, weight_decay in itertools.product(\n",
    "    learning_rates, batch_sizes, optimizers, image_sizes, weight_decays\n",
    "):\n",
    "    # Set momentum only for SGD\n",
    "    momentum = 0.9 if optimizer == 'SGD' else None\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        epochs=15,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        device=0,\n",
    "        lr0=lr,\n",
    "        optimizer=optimizer,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Get the metric to optimize (e.g., mAP@50)\n",
    "    map50 = results.maps\n",
    "\n",
    "    # Log the results\n",
    "    print(f\"lr: {lr}, batch_size: {batch_size}, optimizer: {optimizer}, img_size: {img_size}, weight_decay: {weight_decay}, mAP@50: {map50}\")\n",
    "\n",
    "    # Update the best parameters\n",
    "    if map50 > best_map:\n",
    "        best_map = map50\n",
    "        best_params = {\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'optimizer': optimizer,\n",
    "            'momentum': momentum,\n",
    "            'weight_decay': weight_decay,\n",
    "            'img_size': img_size\n",
    "        }\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best mAP@50:\", best_map)\n",
    "#Application of regularization techniques (e.g., dropout, batch normalization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is there to check if the Annotations are working correctly.\n",
    "def visualize_yolo_annotations(image_path, label_path, class_names=None):\n",
    " \n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_width, img_height = image.size\n",
    "\n",
    "    # Create a plot\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Read the YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse each line in the label file\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])\n",
    "        x_center, y_center, width, height = map(float, parts[1:])\n",
    "\n",
    "        # Convert normalized coordinates to absolute pixel values\n",
    "        x_center *= img_width\n",
    "        y_center *= img_height\n",
    "        width *= img_width\n",
    "        height *= img_height\n",
    "\n",
    "        # Calculate the top-left corner of the bounding box\n",
    "        x_min = x_center - (width / 2)\n",
    "        y_min = y_center - (height / 2)\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min), width, height,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label (if provided)\n",
    "        if class_names:\n",
    "            ax.text(\n",
    "                x_min, y_min - 5, class_names[class_id],\n",
    "                color='red', fontsize=12, backgroundcolor='white'\n",
    "            )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = \"scarecrow_dataset/train/images/train_original_89.png\"\n",
    "label_path = \"scarecrow_dataset/train/labels/train_original_89.txt\"\n",
    "class_names = [\"Bird\"]  #\n",
    "\n",
    "visualize_yolo_annotations(image_path, label_path, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382c74f",
   "metadata": {},
   "source": [
    "## RF-DETR (ANdrea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b74134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_to_coco_split(source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(source_dir, \"annotations.json\")) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    coco = {\n",
    "        \"info\": {\"description\": f\"Converted from {source_dir}\"},\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": 1, \"name\": \"Bird\", \"supercategory\": \"none\"}]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    for image_id, entry in enumerate(annotations, 1):\n",
    "        file_name = entry[\"OriginalFileName\"]\n",
    "        image_path = os.path.join(source_dir, \"images\", file_name)\n",
    "        new_image_path = os.path.join(target_dir, file_name)\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"⚠️ Skipping missing file: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        shutil.copy(image_path, new_image_path)\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": file_name,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        for obj in entry[\"AnnotationData\"]:\n",
    "            if obj[\"Label\"] != \"Bird\":\n",
    "                continue\n",
    "            coords = obj[\"Coordinates\"]\n",
    "            x_min = min(p[\"X\"] for p in coords)\n",
    "            y_min = min(p[\"Y\"] for p in coords)\n",
    "            x_max = max(p[\"X\"] for p in coords)\n",
    "            y_max = max(p[\"Y\"] for p in coords)\n",
    "            width_box = x_max - x_min\n",
    "            height_box = y_max - y_min\n",
    "\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": 1,\n",
    "                \"bbox\": [x_min, y_min, width_box, height_box],\n",
    "                \"area\": width_box * height_box,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    with open(os.path.join(target_dir, \"_annotations.coco.json\"), \"w\") as f:\n",
    "        json.dump(coco, f, indent=4)\n",
    "\n",
    "    print(f\"✅ Created {len(coco['images'])} images in {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all three splits\n",
    "convert_to_coco_split(\"scarecrow_dataset/train\", \"scarecrow_coco_dataset/train\")\n",
    "convert_to_coco_split(\"scarecrow_dataset/val\", \"scarecrow_coco_dataset/valid\")\n",
    "convert_to_coco_split(\"scarecrow_dataset/test\", \"scarecrow_coco_dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9de69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_coco_split(\"bird-detection-farm/train\", \"bird-detection-farm_coco_dataset/train\")\n",
    "convert_to_coco_split(\"bird-detection-farm/valid\", \"bird-detection-farm_coco_dataset/valid\")\n",
    "convert_to_coco_split(\"bird-detection-farm/test\", \"bird-detection-farm_coco_dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13af1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRBase\n",
    "\n",
    "model = RFDETRBase()\n",
    "\n",
    "model.train(dataset_dir='scarecrow_coco_dataset', epochs=10, batch_size=16, grad_accum_steps=1, lr=1e-4, output_dir='rfdetr_output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
